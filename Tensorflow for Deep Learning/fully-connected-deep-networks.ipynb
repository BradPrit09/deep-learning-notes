{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fully Connected Deep Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hakan/.pyenv/versions/miniconda3-latest/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /var/folders/gt/jd9v6_wj1398xf69593kqr700000gn/T/tox21.csv.gz\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "TIMING: featurizing shard 0 took 15.911 s\n",
      "TIMING: dataset construction took 16.289 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.543 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.529 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.332 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.329 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "_, (train, valid, test), _ = dc.molnet.load_tox21()\n",
    "\n",
    "train_X, train_y, train_w = train.X, train.y, train.w\n",
    "valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
    "test_X, test_y, test_w = test.X, test.y, test.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra tasks\n",
    "train_y = train_y[:, 0]\n",
    "valid_y = valid_y[:, 0]\n",
    "test_y = test_y[:, 0]\n",
    "train_w = train_w[:, 0]\n",
    "valid_w = valid_w[:, 0]\n",
    "test_w = test_w[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6264, 1024)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6264,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1024\n",
    "n_hidden = 50\n",
    "learning_rate = .001\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "dropout_prob = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.placeholder(tf.float32, (None, d))\n",
    "    y = tf.placeholder(tf.float32, (None,))\n",
    "    keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"hidden-layer\"):\n",
    "    W_h = tf.Variable(tf.random_normal((d, n_hidden)))\n",
    "    b_h = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "    x_hidden = tf.nn.relu(tf.matmul(x, W_h) + b_h)\n",
    "    # Apply dropout\n",
    "    x_hidden = tf.nn.dropout(x_hidden, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"output\"):\n",
    "    W_o = tf.Variable(tf.random_normal((n_hidden, 1)))\n",
    "    b_o = tf.Variable(tf.random_normal((1,)))\n",
    "    \n",
    "    y_logit = tf.matmul(x_hidden, W_o) + b_o\n",
    "    # the sigmoid gives the class probability of 1\n",
    "    y_one_prob = tf.sigmoid(y_logit)\n",
    "    # Rounding P(y=1) will give the correct prediction.\n",
    "    y_pred = tf.round(y_one_prob)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # Compute the cross-entropy term for each datapoint\n",
    "    y_expand = tf.expand_dims(y, 1)\n",
    "    entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "    # Sum all contributions\n",
    "    l = tf.reduce_sum(entropy)\n",
    "\n",
    "with tf.name_scope(\"optim\"):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\", l)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('/tmp/fcnet-tox21-dropout', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 0, loss: 5115.893066\n",
      "epoch 0, step 1, loss: 4969.027344\n",
      "epoch 0, step 2, loss: 4838.918457\n",
      "epoch 0, step 3, loss: 4810.499512\n",
      "epoch 0, step 4, loss: 5042.317871\n",
      "epoch 0, step 5, loss: 4906.699219\n",
      "epoch 0, step 6, loss: 4586.889160\n",
      "epoch 0, step 7, loss: 4844.467285\n",
      "epoch 0, step 8, loss: 4981.776855\n",
      "epoch 0, step 9, loss: 4379.239258\n",
      "epoch 0, step 10, loss: 4012.468018\n",
      "epoch 0, step 11, loss: 4254.030273\n",
      "epoch 0, step 12, loss: 4332.365723\n",
      "epoch 0, step 13, loss: 4234.933105\n",
      "epoch 0, step 14, loss: 4021.449951\n",
      "epoch 0, step 15, loss: 3774.566895\n",
      "epoch 0, step 16, loss: 3771.882080\n",
      "epoch 0, step 17, loss: 3792.028809\n",
      "epoch 0, step 18, loss: 3790.996582\n",
      "epoch 0, step 19, loss: 4079.286377\n",
      "epoch 0, step 20, loss: 4253.990723\n",
      "epoch 0, step 21, loss: 3850.843994\n",
      "epoch 0, step 22, loss: 3500.473389\n",
      "epoch 0, step 23, loss: 3347.063232\n",
      "epoch 0, step 24, loss: 3531.354004\n",
      "epoch 0, step 25, loss: 3500.452637\n",
      "epoch 0, step 26, loss: 3638.453369\n",
      "epoch 0, step 27, loss: 3408.017578\n",
      "epoch 0, step 28, loss: 3418.856201\n",
      "epoch 0, step 29, loss: 3338.429199\n",
      "epoch 0, step 30, loss: 3496.344238\n",
      "epoch 0, step 31, loss: 3035.767334\n",
      "epoch 0, step 32, loss: 2984.339600\n",
      "epoch 0, step 33, loss: 3015.704834\n",
      "epoch 0, step 34, loss: 3248.365723\n",
      "epoch 0, step 35, loss: 2871.009033\n",
      "epoch 0, step 36, loss: 3235.399658\n",
      "epoch 0, step 37, loss: 2818.176758\n",
      "epoch 0, step 38, loss: 2668.063232\n",
      "epoch 0, step 39, loss: 2721.806641\n",
      "epoch 0, step 40, loss: 2978.487061\n",
      "epoch 0, step 41, loss: 2838.955078\n",
      "epoch 0, step 42, loss: 2391.944336\n",
      "epoch 0, step 43, loss: 2719.592529\n",
      "epoch 0, step 44, loss: 2239.482178\n",
      "epoch 0, step 45, loss: 2590.808594\n",
      "epoch 0, step 46, loss: 2411.810303\n",
      "epoch 0, step 47, loss: 2374.346191\n",
      "epoch 0, step 48, loss: 2413.201416\n",
      "epoch 0, step 49, loss: 2317.165527\n",
      "epoch 0, step 50, loss: 2482.926758\n",
      "epoch 0, step 51, loss: 2171.131104\n",
      "epoch 0, step 52, loss: 2255.009521\n",
      "epoch 0, step 53, loss: 2345.082520\n",
      "epoch 0, step 54, loss: 2113.706543\n",
      "epoch 0, step 55, loss: 2154.159180\n",
      "epoch 0, step 56, loss: 2191.503418\n",
      "epoch 0, step 57, loss: 2191.345459\n",
      "epoch 0, step 58, loss: 1662.267944\n",
      "epoch 0, step 59, loss: 1899.636597\n",
      "epoch 0, step 60, loss: 1737.464844\n",
      "epoch 0, step 61, loss: 1864.482178\n",
      "epoch 0, step 62, loss: 1031.217041\n",
      "epoch 1, step 63, loss: 1903.920776\n",
      "epoch 1, step 64, loss: 1881.138916\n",
      "epoch 1, step 65, loss: 1615.170898\n",
      "epoch 1, step 66, loss: 1560.032104\n",
      "epoch 1, step 67, loss: 1723.013550\n",
      "epoch 1, step 68, loss: 1740.316162\n",
      "epoch 1, step 69, loss: 1721.854614\n",
      "epoch 1, step 70, loss: 1772.911987\n",
      "epoch 1, step 71, loss: 1968.574829\n",
      "epoch 1, step 72, loss: 1500.616821\n",
      "epoch 1, step 73, loss: 1197.595703\n",
      "epoch 1, step 74, loss: 1443.258423\n",
      "epoch 1, step 75, loss: 1621.351440\n",
      "epoch 1, step 76, loss: 1222.032471\n",
      "epoch 1, step 77, loss: 1439.953857\n",
      "epoch 1, step 78, loss: 1380.734985\n",
      "epoch 1, step 79, loss: 1172.359131\n",
      "epoch 1, step 80, loss: 1138.776733\n",
      "epoch 1, step 81, loss: 1320.203613\n",
      "epoch 1, step 82, loss: 1255.876465\n",
      "epoch 1, step 83, loss: 1242.320557\n",
      "epoch 1, step 84, loss: 942.759277\n",
      "epoch 1, step 85, loss: 1056.326660\n",
      "epoch 1, step 86, loss: 889.170654\n",
      "epoch 1, step 87, loss: 1176.679565\n",
      "epoch 1, step 88, loss: 1301.600708\n",
      "epoch 1, step 89, loss: 1130.000610\n",
      "epoch 1, step 90, loss: 1273.244629\n",
      "epoch 1, step 91, loss: 1257.402710\n",
      "epoch 1, step 92, loss: 958.773438\n",
      "epoch 1, step 93, loss: 1287.379150\n",
      "epoch 1, step 94, loss: 1000.983948\n",
      "epoch 1, step 95, loss: 787.165344\n",
      "epoch 1, step 96, loss: 1031.640869\n",
      "epoch 1, step 97, loss: 1039.102661\n",
      "epoch 1, step 98, loss: 842.667053\n",
      "epoch 1, step 99, loss: 998.585754\n",
      "epoch 1, step 100, loss: 837.662231\n",
      "epoch 1, step 101, loss: 914.927490\n",
      "epoch 1, step 102, loss: 952.226440\n",
      "epoch 1, step 103, loss: 917.310486\n",
      "epoch 1, step 104, loss: 916.544312\n",
      "epoch 1, step 105, loss: 682.230469\n",
      "epoch 1, step 106, loss: 915.936646\n",
      "epoch 1, step 107, loss: 657.316956\n",
      "epoch 1, step 108, loss: 698.605408\n",
      "epoch 1, step 109, loss: 768.420959\n",
      "epoch 1, step 110, loss: 661.940735\n",
      "epoch 1, step 111, loss: 860.888611\n",
      "epoch 1, step 112, loss: 724.146484\n",
      "epoch 1, step 113, loss: 728.165527\n",
      "epoch 1, step 114, loss: 933.219910\n",
      "epoch 1, step 115, loss: 744.033630\n",
      "epoch 1, step 116, loss: 862.831848\n",
      "epoch 1, step 117, loss: 679.387512\n",
      "epoch 1, step 118, loss: 606.207825\n",
      "epoch 1, step 119, loss: 830.450195\n",
      "epoch 1, step 120, loss: 732.179321\n",
      "epoch 1, step 121, loss: 525.034302\n",
      "epoch 1, step 122, loss: 515.859192\n",
      "epoch 1, step 123, loss: 646.415649\n",
      "epoch 1, step 124, loss: 506.908051\n",
      "epoch 1, step 125, loss: 444.029358\n",
      "epoch 2, step 126, loss: 536.239685\n",
      "epoch 2, step 127, loss: 751.590515\n",
      "epoch 2, step 128, loss: 485.577087\n",
      "epoch 2, step 129, loss: 529.669617\n",
      "epoch 2, step 130, loss: 593.592834\n",
      "epoch 2, step 131, loss: 673.635559\n",
      "epoch 2, step 132, loss: 726.801697\n",
      "epoch 2, step 133, loss: 526.714111\n",
      "epoch 2, step 134, loss: 778.661377\n",
      "epoch 2, step 135, loss: 585.825012\n",
      "epoch 2, step 136, loss: 396.110443\n",
      "epoch 2, step 137, loss: 592.192688\n",
      "epoch 2, step 138, loss: 633.729736\n",
      "epoch 2, step 139, loss: 437.977966\n",
      "epoch 2, step 140, loss: 681.461914\n",
      "epoch 2, step 141, loss: 732.349182\n",
      "epoch 2, step 142, loss: 473.821442\n",
      "epoch 2, step 143, loss: 445.757996\n",
      "epoch 2, step 144, loss: 591.960327\n",
      "epoch 2, step 145, loss: 424.686279\n",
      "epoch 2, step 146, loss: 590.233459\n",
      "epoch 2, step 147, loss: 277.037018\n",
      "epoch 2, step 148, loss: 436.614563\n",
      "epoch 2, step 149, loss: 325.343018\n",
      "epoch 2, step 150, loss: 576.314636\n",
      "epoch 2, step 151, loss: 702.526306\n",
      "epoch 2, step 152, loss: 386.805420\n",
      "epoch 2, step 153, loss: 670.119629\n",
      "epoch 2, step 154, loss: 547.643066\n",
      "epoch 2, step 155, loss: 331.878876\n",
      "epoch 2, step 156, loss: 584.950684\n",
      "epoch 2, step 157, loss: 430.108978\n",
      "epoch 2, step 158, loss: 301.236877\n",
      "epoch 2, step 159, loss: 461.444122\n",
      "epoch 2, step 160, loss: 460.744629\n",
      "epoch 2, step 161, loss: 385.737732\n",
      "epoch 2, step 162, loss: 381.494080\n",
      "epoch 2, step 163, loss: 341.097321\n",
      "epoch 2, step 164, loss: 478.955872\n",
      "epoch 2, step 165, loss: 535.620850\n",
      "epoch 2, step 166, loss: 390.464386\n",
      "epoch 2, step 167, loss: 433.417236\n",
      "epoch 2, step 168, loss: 322.027252\n",
      "epoch 2, step 169, loss: 460.454956\n",
      "epoch 2, step 170, loss: 375.487000\n",
      "epoch 2, step 171, loss: 282.569275\n",
      "epoch 2, step 172, loss: 359.081299\n",
      "epoch 2, step 173, loss: 320.191956\n",
      "epoch 2, step 174, loss: 516.212646\n",
      "epoch 2, step 175, loss: 336.055206\n",
      "epoch 2, step 176, loss: 299.596497\n",
      "epoch 2, step 177, loss: 567.529175\n",
      "epoch 2, step 178, loss: 402.191040\n",
      "epoch 2, step 179, loss: 463.460449\n",
      "epoch 2, step 180, loss: 307.005493\n",
      "epoch 2, step 181, loss: 242.647964\n",
      "epoch 2, step 182, loss: 482.279480\n",
      "epoch 2, step 183, loss: 398.338043\n",
      "epoch 2, step 184, loss: 379.042725\n",
      "epoch 2, step 185, loss: 244.942841\n",
      "epoch 2, step 186, loss: 439.019592\n",
      "epoch 2, step 187, loss: 245.313141\n",
      "epoch 2, step 188, loss: 281.961273\n",
      "epoch 3, step 189, loss: 222.516449\n",
      "epoch 3, step 190, loss: 440.585358\n",
      "epoch 3, step 191, loss: 271.575714\n",
      "epoch 3, step 192, loss: 307.394348\n",
      "epoch 3, step 193, loss: 335.727570\n",
      "epoch 3, step 194, loss: 397.853973\n",
      "epoch 3, step 195, loss: 440.829895\n",
      "epoch 3, step 196, loss: 206.682709\n",
      "epoch 3, step 197, loss: 427.697388\n",
      "epoch 3, step 198, loss: 356.299469\n",
      "epoch 3, step 199, loss: 223.180313\n",
      "epoch 3, step 200, loss: 347.717712\n",
      "epoch 3, step 201, loss: 393.473511\n",
      "epoch 3, step 202, loss: 295.459015\n",
      "epoch 3, step 203, loss: 443.409882\n",
      "epoch 3, step 204, loss: 495.255463\n",
      "epoch 3, step 205, loss: 302.609589\n",
      "epoch 3, step 206, loss: 292.151886\n",
      "epoch 3, step 207, loss: 441.076447\n",
      "epoch 3, step 208, loss: 215.600143\n",
      "epoch 3, step 209, loss: 406.943878\n",
      "epoch 3, step 210, loss: 150.487427\n",
      "epoch 3, step 211, loss: 261.154388\n",
      "epoch 3, step 212, loss: 197.575790\n",
      "epoch 3, step 213, loss: 406.831207\n",
      "epoch 3, step 214, loss: 470.016388\n",
      "epoch 3, step 215, loss: 183.979416\n",
      "epoch 3, step 216, loss: 454.868958\n",
      "epoch 3, step 217, loss: 348.284485\n",
      "epoch 3, step 218, loss: 171.474899\n",
      "epoch 3, step 219, loss: 376.017151\n",
      "epoch 3, step 220, loss: 299.591949\n",
      "epoch 3, step 221, loss: 191.664856\n",
      "epoch 3, step 222, loss: 292.546326\n",
      "epoch 3, step 223, loss: 282.447174\n",
      "epoch 3, step 224, loss: 267.484039\n",
      "epoch 3, step 225, loss: 209.425064\n",
      "epoch 3, step 226, loss: 200.387436\n",
      "epoch 3, step 227, loss: 334.314575\n",
      "epoch 3, step 228, loss: 383.021271\n",
      "epoch 3, step 229, loss: 242.645096\n",
      "epoch 3, step 230, loss: 290.633392\n",
      "epoch 3, step 231, loss: 219.146576\n",
      "epoch 3, step 232, loss: 286.348541\n",
      "epoch 3, step 233, loss: 290.033905\n",
      "epoch 3, step 234, loss: 156.422256\n",
      "epoch 3, step 235, loss: 201.996918\n",
      "epoch 3, step 236, loss: 208.992981\n",
      "epoch 3, step 237, loss: 393.223785\n",
      "epoch 3, step 238, loss: 205.466431\n",
      "epoch 3, step 239, loss: 170.152832\n",
      "epoch 3, step 240, loss: 398.051422\n",
      "epoch 3, step 241, loss: 276.316742\n",
      "epoch 3, step 242, loss: 301.449738\n",
      "epoch 3, step 243, loss: 199.176712\n",
      "epoch 3, step 244, loss: 138.385986\n",
      "epoch 3, step 245, loss: 329.534515\n",
      "epoch 3, step 246, loss: 281.016724\n",
      "epoch 3, step 247, loss: 334.974670\n",
      "epoch 3, step 248, loss: 176.141052\n",
      "epoch 3, step 249, loss: 362.721924\n",
      "epoch 3, step 250, loss: 171.213608\n",
      "epoch 3, step 251, loss: 212.557663\n",
      "epoch 4, step 252, loss: 128.677094\n",
      "epoch 4, step 253, loss: 303.724365\n",
      "epoch 4, step 254, loss: 197.081680\n",
      "epoch 4, step 255, loss: 238.980759\n",
      "epoch 4, step 256, loss: 270.295715\n",
      "epoch 4, step 257, loss: 281.133087\n",
      "epoch 4, step 258, loss: 333.511414\n",
      "epoch 4, step 259, loss: 119.733421\n",
      "epoch 4, step 260, loss: 275.468292\n",
      "epoch 4, step 261, loss: 250.757980\n",
      "epoch 4, step 262, loss: 170.553009\n",
      "epoch 4, step 263, loss: 259.057678\n",
      "epoch 4, step 264, loss: 311.308289\n",
      "epoch 4, step 265, loss: 241.376755\n",
      "epoch 4, step 266, loss: 347.297882\n",
      "epoch 4, step 267, loss: 374.361786\n",
      "epoch 4, step 268, loss: 229.087738\n",
      "epoch 4, step 269, loss: 238.048325\n",
      "epoch 4, step 270, loss: 385.676361\n",
      "epoch 4, step 271, loss: 135.031052\n",
      "epoch 4, step 272, loss: 330.443634\n",
      "epoch 4, step 273, loss: 110.682281\n",
      "epoch 4, step 274, loss: 204.455078\n",
      "epoch 4, step 275, loss: 132.492706\n",
      "epoch 4, step 276, loss: 301.602936\n",
      "epoch 4, step 277, loss: 347.284302\n",
      "epoch 4, step 278, loss: 121.447289\n",
      "epoch 4, step 279, loss: 341.567352\n",
      "epoch 4, step 280, loss: 260.073700\n",
      "epoch 4, step 281, loss: 105.915947\n",
      "epoch 4, step 282, loss: 286.085632\n",
      "epoch 4, step 283, loss: 247.657761\n",
      "epoch 4, step 284, loss: 149.405411\n",
      "epoch 4, step 285, loss: 217.799667\n",
      "epoch 4, step 286, loss: 207.823792\n",
      "epoch 4, step 287, loss: 214.331146\n",
      "epoch 4, step 288, loss: 155.666733\n",
      "epoch 4, step 289, loss: 131.330933\n",
      "epoch 4, step 290, loss: 262.907471\n",
      "epoch 4, step 291, loss: 286.350281\n",
      "epoch 4, step 292, loss: 173.678848\n",
      "epoch 4, step 293, loss: 225.527695\n",
      "epoch 4, step 294, loss: 177.485489\n",
      "epoch 4, step 295, loss: 213.793564\n",
      "epoch 4, step 296, loss: 251.250061\n",
      "epoch 4, step 297, loss: 111.073563\n",
      "epoch 4, step 298, loss: 123.870728\n",
      "epoch 4, step 299, loss: 146.179871\n",
      "epoch 4, step 300, loss: 317.550385\n",
      "epoch 4, step 301, loss: 146.509003\n",
      "epoch 4, step 302, loss: 112.142906\n",
      "epoch 4, step 303, loss: 291.368164\n",
      "epoch 4, step 304, loss: 207.007156\n",
      "epoch 4, step 305, loss: 210.382126\n",
      "epoch 4, step 306, loss: 137.374207\n",
      "epoch 4, step 307, loss: 88.128029\n",
      "epoch 4, step 308, loss: 230.050980\n",
      "epoch 4, step 309, loss: 220.103043\n",
      "epoch 4, step 310, loss: 300.644806\n",
      "epoch 4, step 311, loss: 141.797485\n",
      "epoch 4, step 312, loss: 315.678375\n",
      "epoch 4, step 313, loss: 130.904617\n",
      "epoch 4, step 314, loss: 169.273834\n",
      "epoch 5, step 315, loss: 94.797302\n",
      "epoch 5, step 316, loss: 222.899963\n",
      "epoch 5, step 317, loss: 159.516342\n",
      "epoch 5, step 318, loss: 211.482895\n",
      "epoch 5, step 319, loss: 241.224670\n",
      "epoch 5, step 320, loss: 220.538010\n",
      "epoch 5, step 321, loss: 275.189972\n",
      "epoch 5, step 322, loss: 89.740646\n",
      "epoch 5, step 323, loss: 187.160919\n",
      "epoch 5, step 324, loss: 191.199326\n",
      "epoch 5, step 325, loss: 144.313782\n",
      "epoch 5, step 326, loss: 198.678940\n",
      "epoch 5, step 327, loss: 260.814758\n",
      "epoch 5, step 328, loss: 209.362183\n",
      "epoch 5, step 329, loss: 293.033722\n",
      "epoch 5, step 330, loss: 282.383118\n",
      "epoch 5, step 331, loss: 175.852844\n",
      "epoch 5, step 332, loss: 197.826187\n",
      "epoch 5, step 333, loss: 356.715393\n",
      "epoch 5, step 334, loss: 100.705849\n",
      "epoch 5, step 335, loss: 276.824646\n",
      "epoch 5, step 336, loss: 91.858047\n",
      "epoch 5, step 337, loss: 178.834351\n",
      "epoch 5, step 338, loss: 83.968582\n",
      "epoch 5, step 339, loss: 220.072006\n",
      "epoch 5, step 340, loss: 258.724823\n",
      "epoch 5, step 341, loss: 90.646820\n",
      "epoch 5, step 342, loss: 266.543671\n",
      "epoch 5, step 343, loss: 192.182022\n",
      "epoch 5, step 344, loss: 76.324127\n",
      "epoch 5, step 345, loss: 232.556091\n",
      "epoch 5, step 346, loss: 215.817459\n",
      "epoch 5, step 347, loss: 126.030289\n",
      "epoch 5, step 348, loss: 166.303253\n",
      "epoch 5, step 349, loss: 170.686447\n",
      "epoch 5, step 350, loss: 184.449585\n",
      "epoch 5, step 351, loss: 127.631500\n",
      "epoch 5, step 352, loss: 103.525848\n",
      "epoch 5, step 353, loss: 206.606674\n",
      "epoch 5, step 354, loss: 213.356964\n",
      "epoch 5, step 355, loss: 122.202354\n",
      "epoch 5, step 356, loss: 189.994415\n",
      "epoch 5, step 357, loss: 160.045654\n",
      "epoch 5, step 358, loss: 183.657745\n",
      "epoch 5, step 359, loss: 237.040787\n",
      "epoch 5, step 360, loss: 95.406715\n",
      "epoch 5, step 361, loss: 75.482910\n",
      "epoch 5, step 362, loss: 98.590408\n",
      "epoch 5, step 363, loss: 260.522797\n",
      "epoch 5, step 364, loss: 116.479942\n",
      "epoch 5, step 365, loss: 82.672188\n",
      "epoch 5, step 366, loss: 196.782379\n",
      "epoch 5, step 367, loss: 156.802444\n",
      "epoch 5, step 368, loss: 146.548096\n",
      "epoch 5, step 369, loss: 105.869385\n",
      "epoch 5, step 370, loss: 55.575306\n",
      "epoch 5, step 371, loss: 153.792679\n",
      "epoch 5, step 372, loss: 183.822357\n",
      "epoch 5, step 373, loss: 268.385193\n",
      "epoch 5, step 374, loss: 122.039375\n",
      "epoch 5, step 375, loss: 274.517487\n",
      "epoch 5, step 376, loss: 114.197662\n",
      "epoch 5, step 377, loss: 136.677628\n",
      "epoch 6, step 378, loss: 80.751709\n",
      "epoch 6, step 379, loss: 169.783401\n",
      "epoch 6, step 380, loss: 135.062302\n",
      "epoch 6, step 381, loss: 188.800705\n",
      "epoch 6, step 382, loss: 220.592545\n",
      "epoch 6, step 383, loss: 181.329285\n",
      "epoch 6, step 384, loss: 227.808609\n",
      "epoch 6, step 385, loss: 76.736725\n",
      "epoch 6, step 386, loss: 125.045166\n",
      "epoch 6, step 387, loss: 142.604614\n",
      "epoch 6, step 388, loss: 128.673233\n",
      "epoch 6, step 389, loss: 153.947906\n",
      "epoch 6, step 390, loss: 217.674805\n",
      "epoch 6, step 391, loss: 188.991638\n",
      "epoch 6, step 392, loss: 253.758926\n",
      "epoch 6, step 393, loss: 211.459534\n",
      "epoch 6, step 394, loss: 130.367081\n",
      "epoch 6, step 395, loss: 161.512833\n",
      "epoch 6, step 396, loss: 330.576782\n",
      "epoch 6, step 397, loss: 85.498085\n",
      "epoch 6, step 398, loss: 245.645493\n",
      "epoch 6, step 399, loss: 80.167892\n",
      "epoch 6, step 400, loss: 161.685379\n",
      "epoch 6, step 401, loss: 53.397003\n",
      "epoch 6, step 402, loss: 159.552048\n",
      "epoch 6, step 403, loss: 187.624680\n",
      "epoch 6, step 404, loss: 73.050461\n",
      "epoch 6, step 405, loss: 213.059723\n",
      "epoch 6, step 406, loss: 138.559448\n",
      "epoch 6, step 407, loss: 61.285465\n",
      "epoch 6, step 408, loss: 198.117477\n",
      "epoch 6, step 409, loss: 191.284164\n",
      "epoch 6, step 410, loss: 114.310814\n",
      "epoch 6, step 411, loss: 126.659103\n",
      "epoch 6, step 412, loss: 154.421890\n",
      "epoch 6, step 413, loss: 165.203278\n",
      "epoch 6, step 414, loss: 107.848801\n",
      "epoch 6, step 415, loss: 91.860291\n",
      "epoch 6, step 416, loss: 164.921295\n",
      "epoch 6, step 417, loss: 159.652649\n",
      "epoch 6, step 418, loss: 81.019402\n",
      "epoch 6, step 419, loss: 159.551361\n",
      "epoch 6, step 420, loss: 148.558258\n",
      "epoch 6, step 421, loss: 164.467209\n",
      "epoch 6, step 422, loss: 229.006424\n",
      "epoch 6, step 423, loss: 88.435440\n",
      "epoch 6, step 424, loss: 40.189533\n",
      "epoch 6, step 425, loss: 72.919952\n",
      "epoch 6, step 426, loss: 212.800644\n",
      "epoch 6, step 427, loss: 95.372292\n",
      "epoch 6, step 428, loss: 63.073448\n",
      "epoch 6, step 429, loss: 121.978996\n",
      "epoch 6, step 430, loss: 117.285156\n",
      "epoch 6, step 431, loss: 93.280365\n",
      "epoch 6, step 432, loss: 99.620697\n",
      "epoch 6, step 433, loss: 39.935856\n",
      "epoch 6, step 434, loss: 98.685020\n",
      "epoch 6, step 435, loss: 162.256912\n",
      "epoch 6, step 436, loss: 241.860367\n",
      "epoch 6, step 437, loss: 108.787399\n",
      "epoch 6, step 438, loss: 239.490829\n",
      "epoch 6, step 439, loss: 106.033936\n",
      "epoch 6, step 440, loss: 107.621239\n",
      "epoch 7, step 441, loss: 72.636375\n",
      "epoch 7, step 442, loss: 129.900696\n",
      "epoch 7, step 443, loss: 117.249084\n",
      "epoch 7, step 444, loss: 169.363388\n",
      "epoch 7, step 445, loss: 205.067429\n",
      "epoch 7, step 446, loss: 157.161713\n",
      "epoch 7, step 447, loss: 187.694809\n",
      "epoch 7, step 448, loss: 68.564278\n",
      "epoch 7, step 449, loss: 80.608147\n",
      "epoch 7, step 450, loss: 100.793602\n",
      "epoch 7, step 451, loss: 118.498207\n",
      "epoch 7, step 452, loss: 123.145638\n",
      "epoch 7, step 453, loss: 177.964142\n",
      "epoch 7, step 454, loss: 171.911911\n",
      "epoch 7, step 455, loss: 221.479294\n",
      "epoch 7, step 456, loss: 161.484314\n",
      "epoch 7, step 457, loss: 89.813644\n",
      "epoch 7, step 458, loss: 136.431992\n",
      "epoch 7, step 459, loss: 304.276520\n",
      "epoch 7, step 460, loss: 75.594788\n",
      "epoch 7, step 461, loss: 219.925339\n",
      "epoch 7, step 462, loss: 70.683388\n",
      "epoch 7, step 463, loss: 148.909485\n",
      "epoch 7, step 464, loss: 32.959969\n",
      "epoch 7, step 465, loss: 116.152969\n",
      "epoch 7, step 466, loss: 128.456146\n",
      "epoch 7, step 467, loss: 64.097664\n",
      "epoch 7, step 468, loss: 172.134384\n",
      "epoch 7, step 469, loss: 94.864738\n",
      "epoch 7, step 470, loss: 51.114902\n",
      "epoch 7, step 471, loss: 173.509628\n",
      "epoch 7, step 472, loss: 171.604828\n",
      "epoch 7, step 473, loss: 106.629189\n",
      "epoch 7, step 474, loss: 101.329010\n",
      "epoch 7, step 475, loss: 142.686966\n",
      "epoch 7, step 476, loss: 150.156509\n",
      "epoch 7, step 477, loss: 93.670525\n",
      "epoch 7, step 478, loss: 83.631592\n",
      "epoch 7, step 479, loss: 135.504883\n",
      "epoch 7, step 480, loss: 126.473495\n",
      "epoch 7, step 481, loss: 52.388474\n",
      "epoch 7, step 482, loss: 136.560364\n",
      "epoch 7, step 483, loss: 139.001450\n",
      "epoch 7, step 484, loss: 148.833160\n",
      "epoch 7, step 485, loss: 221.209961\n",
      "epoch 7, step 486, loss: 83.265564\n",
      "epoch 7, step 487, loss: 29.184769\n",
      "epoch 7, step 488, loss: 68.023621\n",
      "epoch 7, step 489, loss: 176.253662\n",
      "epoch 7, step 490, loss: 79.556679\n",
      "epoch 7, step 491, loss: 47.956738\n",
      "epoch 7, step 492, loss: 73.071236\n",
      "epoch 7, step 493, loss: 86.606094\n",
      "epoch 7, step 494, loss: 60.874416\n",
      "epoch 7, step 495, loss: 99.461678\n",
      "epoch 7, step 496, loss: 32.488369\n",
      "epoch 7, step 497, loss: 67.416168\n",
      "epoch 7, step 498, loss: 144.214233\n",
      "epoch 7, step 499, loss: 221.421326\n",
      "epoch 7, step 500, loss: 100.081779\n",
      "epoch 7, step 501, loss: 212.267792\n",
      "epoch 7, step 502, loss: 100.134880\n",
      "epoch 7, step 503, loss: 97.891129\n",
      "epoch 8, step 504, loss: 67.490540\n",
      "epoch 8, step 505, loss: 102.227295\n",
      "epoch 8, step 506, loss: 107.615929\n",
      "epoch 8, step 507, loss: 155.451981\n",
      "epoch 8, step 508, loss: 193.673157\n",
      "epoch 8, step 509, loss: 142.705154\n",
      "epoch 8, step 510, loss: 156.359894\n",
      "epoch 8, step 511, loss: 62.273911\n",
      "epoch 8, step 512, loss: 61.445896\n",
      "epoch 8, step 513, loss: 71.422302\n",
      "epoch 8, step 514, loss: 111.236320\n",
      "epoch 8, step 515, loss: 106.039017\n",
      "epoch 8, step 516, loss: 146.748917\n",
      "epoch 8, step 517, loss: 161.412964\n",
      "epoch 8, step 518, loss: 197.913208\n",
      "epoch 8, step 519, loss: 129.355103\n",
      "epoch 8, step 520, loss: 67.747772\n",
      "epoch 8, step 521, loss: 130.415756\n",
      "epoch 8, step 522, loss: 282.444641\n",
      "epoch 8, step 523, loss: 68.468468\n",
      "epoch 8, step 524, loss: 203.524002\n",
      "epoch 8, step 525, loss: 62.372185\n",
      "epoch 8, step 526, loss: 138.652771\n",
      "epoch 8, step 527, loss: 29.792336\n",
      "epoch 8, step 528, loss: 85.749245\n",
      "epoch 8, step 529, loss: 88.484169\n",
      "epoch 8, step 530, loss: 58.103237\n",
      "epoch 8, step 531, loss: 148.122360\n",
      "epoch 8, step 532, loss: 63.362823\n",
      "epoch 8, step 533, loss: 42.791611\n",
      "epoch 8, step 534, loss: 155.107452\n",
      "epoch 8, step 535, loss: 156.794098\n",
      "epoch 8, step 536, loss: 100.864754\n",
      "epoch 8, step 537, loss: 90.789520\n",
      "epoch 8, step 538, loss: 134.624664\n",
      "epoch 8, step 539, loss: 138.163147\n",
      "epoch 8, step 540, loss: 83.738632\n",
      "epoch 8, step 541, loss: 78.254913\n",
      "epoch 8, step 542, loss: 116.327812\n",
      "epoch 8, step 543, loss: 105.003548\n",
      "epoch 8, step 544, loss: 39.801651\n",
      "epoch 8, step 545, loss: 124.673241\n",
      "epoch 8, step 546, loss: 130.519180\n",
      "epoch 8, step 547, loss: 134.970886\n",
      "epoch 8, step 548, loss: 211.576508\n",
      "epoch 8, step 549, loss: 78.600586\n",
      "epoch 8, step 550, loss: 26.149508\n",
      "epoch 8, step 551, loss: 66.998848\n",
      "epoch 8, step 552, loss: 154.143219\n",
      "epoch 8, step 553, loss: 66.972870\n",
      "epoch 8, step 554, loss: 36.976719\n",
      "epoch 8, step 555, loss: 49.775818\n",
      "epoch 8, step 556, loss: 63.953094\n",
      "epoch 8, step 557, loss: 48.094337\n",
      "epoch 8, step 558, loss: 97.054924\n",
      "epoch 8, step 559, loss: 29.955877\n",
      "epoch 8, step 560, loss: 48.607231\n",
      "epoch 8, step 561, loss: 128.613037\n",
      "epoch 8, step 562, loss: 212.062210\n",
      "epoch 8, step 563, loss: 94.229347\n",
      "epoch 8, step 564, loss: 193.063568\n",
      "epoch 8, step 565, loss: 95.274719\n",
      "epoch 8, step 566, loss: 91.473465\n",
      "epoch 9, step 567, loss: 63.608776\n",
      "epoch 9, step 568, loss: 83.667076\n",
      "epoch 9, step 569, loss: 101.152458\n",
      "epoch 9, step 570, loss: 144.209717\n",
      "epoch 9, step 571, loss: 184.410767\n",
      "epoch 9, step 572, loss: 134.828537\n",
      "epoch 9, step 573, loss: 133.721634\n",
      "epoch 9, step 574, loss: 57.182503\n",
      "epoch 9, step 575, loss: 49.926670\n",
      "epoch 9, step 576, loss: 65.463837\n",
      "epoch 9, step 577, loss: 105.800613\n",
      "epoch 9, step 578, loss: 93.837128\n",
      "epoch 9, step 579, loss: 122.957932\n",
      "epoch 9, step 580, loss: 158.636566\n",
      "epoch 9, step 581, loss: 181.835342\n",
      "epoch 9, step 582, loss: 111.422577\n",
      "epoch 9, step 583, loss: 53.865677\n",
      "epoch 9, step 584, loss: 125.031784\n",
      "epoch 9, step 585, loss: 264.508057\n",
      "epoch 9, step 586, loss: 63.708611\n",
      "epoch 9, step 587, loss: 192.719360\n",
      "epoch 9, step 588, loss: 54.789341\n",
      "epoch 9, step 589, loss: 130.021805\n",
      "epoch 9, step 590, loss: 27.830837\n",
      "epoch 9, step 591, loss: 70.211098\n",
      "epoch 9, step 592, loss: 66.313469\n",
      "epoch 9, step 593, loss: 54.450722\n",
      "epoch 9, step 594, loss: 133.527863\n",
      "epoch 9, step 595, loss: 45.454266\n",
      "epoch 9, step 596, loss: 35.711281\n",
      "epoch 9, step 597, loss: 140.916183\n",
      "epoch 9, step 598, loss: 146.465698\n",
      "epoch 9, step 599, loss: 99.432648\n",
      "epoch 9, step 600, loss: 82.920128\n",
      "epoch 9, step 601, loss: 127.820702\n",
      "epoch 9, step 602, loss: 128.083572\n",
      "epoch 9, step 603, loss: 75.703835\n",
      "epoch 9, step 604, loss: 74.277290\n",
      "epoch 9, step 605, loss: 104.402893\n",
      "epoch 9, step 606, loss: 88.459747\n",
      "epoch 9, step 607, loss: 33.727646\n",
      "epoch 9, step 608, loss: 114.973610\n",
      "epoch 9, step 609, loss: 123.163353\n",
      "epoch 9, step 610, loss: 123.452377\n",
      "epoch 9, step 611, loss: 203.201538\n",
      "epoch 9, step 612, loss: 74.008820\n",
      "epoch 9, step 613, loss: 22.933727\n",
      "epoch 9, step 614, loss: 64.490799\n",
      "epoch 9, step 615, loss: 142.948196\n",
      "epoch 9, step 616, loss: 58.264816\n",
      "epoch 9, step 617, loss: 29.662264\n",
      "epoch 9, step 618, loss: 40.416683\n",
      "epoch 9, step 619, loss: 48.504101\n",
      "epoch 9, step 620, loss: 40.093838\n",
      "epoch 9, step 621, loss: 91.321426\n",
      "epoch 9, step 622, loss: 29.373566\n",
      "epoch 9, step 623, loss: 37.206139\n",
      "epoch 9, step 624, loss: 115.399734\n",
      "epoch 9, step 625, loss: 204.977493\n",
      "epoch 9, step 626, loss: 90.358757\n",
      "epoch 9, step 627, loss: 180.221497\n",
      "epoch 9, step 628, loss: 91.588600\n",
      "epoch 9, step 629, loss: 85.920700\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "N = train_X.shape[0]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        pos = 0\n",
    "        while pos < N:\n",
    "            batch_X = train_X[pos:pos+batch_size]\n",
    "            batch_y = train_y[pos:pos+batch_size]\n",
    "\n",
    "            feed_dict = {x: batch_X, y: batch_y, keep_prob: dropout_prob}\n",
    "            _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "\n",
    "            print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "            step += 1\n",
    "            pos += batch_size\n",
    "      \n",
    "    # Make Predictions (set keep_prob to 1.0 for predictions)\n",
    "    train_y_pred = sess.run(y_pred, feed_dict={x: train_X, keep_prob: 1.0})\n",
    "    valid_y_pred = sess.run(y_pred, feed_dict={x: valid_X, keep_prob: 1.0})\n",
    "    test_y_pred = sess.run(y_pred, feed_dict={x: test_X, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Weighted Classification Accuracy: 0.647355\n",
      "Valid Weighted Classification Accuracy: 0.602728\n"
     ]
    }
   ],
   "source": [
    "train_weighted_score = accuracy_score(train_y, train_y_pred, sample_weight=train_w)\n",
    "print(\"Train Weighted Classification Accuracy: %f\" % train_weighted_score)\n",
    "valid_weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "print(\"Valid Weighted Classification Accuracy: %f\" % valid_weighted_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
