{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3. Linear and Logistic Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hakan/.pyenv/versions/3.6.4/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow **without** Eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2.]\n",
      " [2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones((2, 2))\n",
    "b = tf.matmul(a, a)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow with Eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9f75839e21bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph_mode_has_been_used\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5445\u001b[0m       raise ValueError(\n\u001b[0;32m-> 5446\u001b[0;31m           \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5447\u001b[0m   \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEAGER_MODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5448\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "\n",
    "tf.executing_eagerly()      \n",
    "a = tf.ones((2, 2))\n",
    "b = tf.matmul(a, a)\n",
    "print(b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "N = 100\n",
    "w_true = 5\n",
    "b_true = 2\n",
    "noise_scale = .1\n",
    "x_np = np.random.rand(N, 1)\n",
    "\n",
    "noise = np.random.normal(scale=noise_scale, size=(N, 1))\n",
    "# Convert shape of y_np to (N,)\n",
    "y_np = np.reshape(w_true * x_np + b_true + noise, (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.18175572, 2.83886481, 6.47964086, 3.01911277, 6.74831299,\n",
       "       5.48370025, 5.46360334, 2.38200281, 6.85954026, 6.13493042])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_np[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04065246],\n",
       "       [ 0.02093832],\n",
       "       [ 0.05628597],\n",
       "       [ 0.16619514],\n",
       "       [-0.06617736],\n",
       "       [ 0.02862734],\n",
       "       [-0.12137265],\n",
       "       [-0.1068516 ],\n",
       "       [ 0.10262571],\n",
       "       [ 0.01358497]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save image of the data distribution\n",
    "\n",
    "plt.scatter(x_np, y_np)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim(0, 1)\n",
    "plt.title(\"Toy Linear Regression Data, \"r\"$y = 5x + 2 + N(0, 1)$\")\n",
    "plt.savefig(\"lr_data.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "N = 100\n",
    "\n",
    "# Zeros form a Gaussian centered at (-1, -1)\n",
    "# epsilon is .1\n",
    "x_zeros = np.random.multivariate_normal(mean=np.array((-1, -1), dtype=int), cov=np.eye(2, dtype=int), size=(N//2,))\n",
    "y_zeros = np.zeros((N//2,))\n",
    "\n",
    "# Ones form a Gaussian centered at (1, 1)\n",
    "# epsilon is .1\n",
    "x_ones = np.random.multivariate_normal(mean=np.array((1, 1)), cov=.1*np.eye(2), size=(N//2,))\n",
    "y_ones = np.ones((N//2,))\n",
    "\n",
    "x_np = np.vstack([x_zeros, x_ones])\n",
    "y_np = np.concatenate([y_zeros, y_ones])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_zeros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_zeros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Zeros\n",
    "plt.scatter(x_zeros[:, 0], x_zeros[:, 1], color=\"blue\")\n",
    "plt.scatter(x_ones[:, 0], x_ones[:, 1], color=\"red\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Toy Linear Regression Data\")\n",
    "\n",
    "plt.savefig(\"logistic_data.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New TensorFlow Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(2, 2) dtype=float32>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.placeholder(tf.float32, shape=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32, shape=(1,))\n",
    "b = tf.placeholder(tf.float32, shape=(1,))\n",
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_2:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    c_eval = sess.run(c, {a: [1.], b: [2.]})\n",
    "    print(c_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'add' type=Add>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    c_eval = c.eval({a: [1.], b: [2.]})\n",
    "    print(c_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'placeholders/Placeholder:0' shape=(5, 1) dtype=float32>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.placeholder(tf.float32, (N, 1))\n",
    "    y = tf.placeholder(tf.float32, (N,))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .001\n",
    "with tf.name_scope(\"optim\"):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'gradients/Sum_grad/Tile:0' shape=(1,) dtype=int32>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = tf.Variable((3,))\n",
    ">>> l = tf.reduce_sum(W)\n",
    ">>> gradW = tf.gradients(l, W)\n",
    ">>> gradW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARIES AND FILE WRITERS FOR TENSORBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\", l)\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('/tmp/lr-train', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING MODELS WITH TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Train model\n",
    "    for i in range(n_steps):\n",
    "        feed_dict = {x: x_np, y: y_np}\n",
    "        _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "        print(\"step %d, loss: %f\" % (i, loss))\n",
    "        train_writer.add_summary(summary, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Linear and Logistic Models in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'random_normal:0' shape=(1, 1) dtype=float32>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random_normal((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tensorflow graph\n",
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.placeholder(tf.float32, (N, 2))\n",
    "    y = tf.placeholder(tf.float32, (N,))\n",
    "\n",
    "with tf.name_scope(\"weights\"):\n",
    "    # Note that x is a scalar, so W is a single learnable weight.\n",
    "    W = tf.Variable(tf.random_normal((2, 1)))\n",
    "    b = tf.Variable(tf.random_normal((1,)))\n",
    "\n",
    "with tf.name_scope(\"prediction\"):\n",
    "    y_pred = tf.matmul(x, W) + b\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    l = tf.reduce_sum((y - y_pred)**2)\n",
    "    \n",
    "# Add training op\n",
    "with tf.name_scope(\"optim\"):\n",
    "    # Set learning rate to .001 as recommended above.\n",
    "    train_op = tf.train.AdamOptimizer(.001).minimize(l)\n",
    "    \n",
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\", l)\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('/tmp/lr-train', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_r2_score(y, y_pred):\n",
    "    \"\"\"Computes Pearson R^2 (square of Pearson correlation).\"\"\"\n",
    "    return pearsonr(y, y_pred)[0]**2\n",
    "\n",
    "def rms_score(y_true, y_pred):\n",
    "    \"\"\"Computes RMS error.\"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 47342.718750\n",
      "step 1, loss: 47238.875000\n",
      "step 2, loss: 47135.312500\n",
      "step 3, loss: 47031.683594\n",
      "step 4, loss: 46928.187500\n",
      "step 5, loss: 46824.960938\n",
      "step 6, loss: 46721.820312\n",
      "step 7, loss: 46618.703125\n",
      "step 8, loss: 46515.835938\n",
      "step 9, loss: 46413.195312\n",
      "step 10, loss: 46310.519531\n",
      "step 11, loss: 46208.253906\n",
      "step 12, loss: 46105.742188\n",
      "step 13, loss: 46003.765625\n",
      "step 14, loss: 45901.816406\n",
      "step 15, loss: 45800.042969\n",
      "step 16, loss: 45698.449219\n",
      "step 17, loss: 45596.921875\n",
      "step 18, loss: 45495.687500\n",
      "step 19, loss: 45394.257812\n",
      "step 20, loss: 45293.238281\n",
      "step 21, loss: 45192.515625\n",
      "step 22, loss: 45091.867188\n",
      "step 23, loss: 44991.296875\n",
      "step 24, loss: 44891.042969\n",
      "step 25, loss: 44790.937500\n",
      "step 26, loss: 44690.800781\n",
      "step 27, loss: 44591.062500\n",
      "step 28, loss: 44491.300781\n",
      "step 29, loss: 44391.828125\n",
      "step 30, loss: 44292.421875\n",
      "step 31, loss: 44193.484375\n",
      "step 32, loss: 44094.328125\n",
      "step 33, loss: 43995.656250\n",
      "step 34, loss: 43897.062500\n",
      "step 35, loss: 43798.820312\n",
      "step 36, loss: 43700.429688\n",
      "step 37, loss: 43602.238281\n",
      "step 38, loss: 43504.578125\n",
      "step 39, loss: 43406.781250\n",
      "step 40, loss: 43309.250000\n",
      "step 41, loss: 43211.839844\n",
      "step 42, loss: 43114.750000\n",
      "step 43, loss: 43017.707031\n",
      "step 44, loss: 42921.078125\n",
      "step 45, loss: 42824.390625\n",
      "step 46, loss: 42727.730469\n",
      "step 47, loss: 42631.589844\n",
      "step 48, loss: 42535.570312\n",
      "step 49, loss: 42439.523438\n",
      "step 50, loss: 42344.027344\n",
      "step 51, loss: 42248.292969\n",
      "step 52, loss: 42152.906250\n",
      "step 53, loss: 42057.816406\n",
      "step 54, loss: 41962.816406\n",
      "step 55, loss: 41867.882812\n",
      "step 56, loss: 41773.304688\n",
      "step 57, loss: 41678.828125\n",
      "step 58, loss: 41584.585938\n",
      "step 59, loss: 41490.500000\n",
      "step 60, loss: 41396.566406\n",
      "step 61, loss: 41302.781250\n",
      "step 62, loss: 41209.101562\n",
      "step 63, loss: 41115.812500\n",
      "step 64, loss: 41022.628906\n",
      "step 65, loss: 40929.574219\n",
      "step 66, loss: 40836.812500\n",
      "step 67, loss: 40744.105469\n",
      "step 68, loss: 40651.550781\n",
      "step 69, loss: 40559.371094\n",
      "step 70, loss: 40467.125000\n",
      "step 71, loss: 40375.171875\n",
      "step 72, loss: 40283.218750\n",
      "step 73, loss: 40191.781250\n",
      "step 74, loss: 40100.414062\n",
      "step 75, loss: 40009.234375\n",
      "step 76, loss: 39918.015625\n",
      "step 77, loss: 39827.234375\n",
      "step 78, loss: 39736.539062\n",
      "step 79, loss: 39646.054688\n",
      "step 80, loss: 39555.699219\n",
      "step 81, loss: 39465.437500\n",
      "step 82, loss: 39375.453125\n",
      "step 83, loss: 39285.609375\n",
      "step 84, loss: 39195.902344\n",
      "step 85, loss: 39106.500000\n",
      "step 86, loss: 39017.289062\n",
      "step 87, loss: 38928.019531\n",
      "step 88, loss: 38839.054688\n",
      "step 89, loss: 38750.343750\n",
      "step 90, loss: 38661.734375\n",
      "step 91, loss: 38573.312500\n",
      "step 92, loss: 38484.945312\n",
      "step 93, loss: 38397.031250\n",
      "step 94, loss: 38309.046875\n",
      "step 95, loss: 38221.363281\n",
      "step 96, loss: 38133.765625\n",
      "step 97, loss: 38046.339844\n",
      "step 98, loss: 37959.187500\n",
      "step 99, loss: 37872.058594\n",
      "step 100, loss: 37785.210938\n",
      "step 101, loss: 37698.570312\n",
      "step 102, loss: 37612.042969\n",
      "step 103, loss: 37525.687500\n",
      "step 104, loss: 37439.445312\n",
      "step 105, loss: 37353.542969\n",
      "step 106, loss: 37267.679688\n",
      "step 107, loss: 37182.031250\n",
      "step 108, loss: 37096.429688\n",
      "step 109, loss: 37011.062500\n",
      "step 110, loss: 36926.007812\n",
      "step 111, loss: 36840.898438\n",
      "step 112, loss: 36756.187500\n",
      "step 113, loss: 36671.484375\n",
      "step 114, loss: 36586.992188\n",
      "step 115, loss: 36502.656250\n",
      "step 116, loss: 36418.644531\n",
      "step 117, loss: 36334.644531\n",
      "step 118, loss: 36250.761719\n",
      "step 119, loss: 36167.015625\n",
      "step 120, loss: 36083.597656\n",
      "step 121, loss: 36000.308594\n",
      "step 122, loss: 35917.210938\n",
      "step 123, loss: 35834.218750\n",
      "step 124, loss: 35751.421875\n",
      "step 125, loss: 35668.769531\n",
      "step 126, loss: 35586.382812\n",
      "step 127, loss: 35504.019531\n",
      "step 128, loss: 35421.851562\n",
      "step 129, loss: 35339.875000\n",
      "step 130, loss: 35258.121094\n",
      "step 131, loss: 35176.464844\n",
      "step 132, loss: 35094.964844\n",
      "step 133, loss: 35013.628906\n",
      "step 134, loss: 34932.527344\n",
      "step 135, loss: 34851.507812\n",
      "step 136, loss: 34770.691406\n",
      "step 137, loss: 34690.132812\n",
      "step 138, loss: 34609.500000\n",
      "step 139, loss: 34529.218750\n",
      "step 140, loss: 34449.058594\n",
      "step 141, loss: 34369.039062\n",
      "step 142, loss: 34289.121094\n",
      "step 143, loss: 34209.542969\n",
      "step 144, loss: 34130.023438\n",
      "step 145, loss: 34050.621094\n",
      "step 146, loss: 33971.398438\n",
      "step 147, loss: 33892.410156\n",
      "step 148, loss: 33813.535156\n",
      "step 149, loss: 33734.906250\n",
      "step 150, loss: 33656.257812\n",
      "step 151, loss: 33577.984375\n",
      "step 152, loss: 33499.660156\n",
      "step 153, loss: 33421.738281\n",
      "step 154, loss: 33343.742188\n",
      "step 155, loss: 33265.953125\n",
      "step 156, loss: 33188.359375\n",
      "step 157, loss: 33110.996094\n",
      "step 158, loss: 33033.691406\n",
      "step 159, loss: 32956.507812\n",
      "step 160, loss: 32879.664062\n",
      "step 161, loss: 32802.851562\n",
      "step 162, loss: 32726.164062\n",
      "step 163, loss: 32649.818359\n",
      "step 164, loss: 32573.304688\n",
      "step 165, loss: 32497.273438\n",
      "step 166, loss: 32421.312500\n",
      "step 167, loss: 32345.371094\n",
      "step 168, loss: 32269.728516\n",
      "step 169, loss: 32194.246094\n",
      "step 170, loss: 32118.945312\n",
      "step 171, loss: 32043.548828\n",
      "step 172, loss: 31968.617188\n",
      "step 173, loss: 31893.619141\n",
      "step 174, loss: 31818.917969\n",
      "step 175, loss: 31744.378906\n",
      "step 176, loss: 31669.937500\n",
      "step 177, loss: 31595.550781\n",
      "step 178, loss: 31521.468750\n",
      "step 179, loss: 31447.515625\n",
      "step 180, loss: 31373.642578\n",
      "step 181, loss: 31300.097656\n",
      "step 182, loss: 31226.429688\n",
      "step 183, loss: 31153.156250\n",
      "step 184, loss: 31079.923828\n",
      "step 185, loss: 31006.986328\n",
      "step 186, loss: 30934.039062\n",
      "step 187, loss: 30861.228516\n",
      "step 188, loss: 30788.650391\n",
      "step 189, loss: 30716.343750\n",
      "step 190, loss: 30643.964844\n",
      "step 191, loss: 30571.835938\n",
      "step 192, loss: 30499.882812\n",
      "step 193, loss: 30427.992188\n",
      "step 194, loss: 30356.242188\n",
      "step 195, loss: 30284.744141\n",
      "step 196, loss: 30213.433594\n",
      "step 197, loss: 30142.193359\n",
      "step 198, loss: 30071.103516\n",
      "step 199, loss: 30000.246094\n",
      "step 200, loss: 29929.429688\n",
      "step 201, loss: 29858.875000\n",
      "step 202, loss: 29788.359375\n",
      "step 203, loss: 29717.931641\n",
      "step 204, loss: 29647.738281\n",
      "step 205, loss: 29577.843750\n",
      "step 206, loss: 29507.923828\n",
      "step 207, loss: 29438.251953\n",
      "step 208, loss: 29368.546875\n",
      "step 209, loss: 29299.144531\n",
      "step 210, loss: 29229.816406\n",
      "step 211, loss: 29160.769531\n",
      "step 212, loss: 29091.732422\n",
      "step 213, loss: 29022.833984\n",
      "step 214, loss: 28954.207031\n",
      "step 215, loss: 28885.707031\n",
      "step 216, loss: 28817.251953\n",
      "step 217, loss: 28748.951172\n",
      "step 218, loss: 28680.832031\n",
      "step 219, loss: 28612.855469\n",
      "step 220, loss: 28545.134766\n",
      "step 221, loss: 28477.414062\n",
      "step 222, loss: 28409.908203\n",
      "step 223, loss: 28342.523438\n",
      "step 224, loss: 28275.242188\n",
      "step 225, loss: 28208.171875\n",
      "step 226, loss: 28141.148438\n",
      "step 227, loss: 28074.404297\n",
      "step 228, loss: 28007.757812\n",
      "step 229, loss: 27941.269531\n",
      "step 230, loss: 27874.833984\n",
      "step 231, loss: 27808.699219\n",
      "step 232, loss: 27742.525391\n",
      "step 233, loss: 27676.554688\n",
      "step 234, loss: 27610.771484\n",
      "step 235, loss: 27545.121094\n",
      "step 236, loss: 27479.509766\n",
      "step 237, loss: 27414.222656\n",
      "step 238, loss: 27349.060547\n",
      "step 239, loss: 27283.894531\n",
      "step 240, loss: 27218.925781\n",
      "step 241, loss: 27154.142578\n",
      "step 242, loss: 27089.457031\n",
      "step 243, loss: 27024.859375\n",
      "step 244, loss: 26960.519531\n",
      "step 245, loss: 26896.339844\n",
      "step 246, loss: 26832.232422\n",
      "step 247, loss: 26768.234375\n",
      "step 248, loss: 26704.425781\n",
      "step 249, loss: 26640.677734\n",
      "step 250, loss: 26577.189453\n",
      "step 251, loss: 26513.734375\n",
      "step 252, loss: 26450.517578\n",
      "step 253, loss: 26387.425781\n",
      "step 254, loss: 26324.375000\n",
      "step 255, loss: 26261.511719\n",
      "step 256, loss: 26198.798828\n",
      "step 257, loss: 26136.189453\n",
      "step 258, loss: 26073.712891\n",
      "step 259, loss: 26011.482422\n",
      "step 260, loss: 25949.281250\n",
      "step 261, loss: 25887.296875\n",
      "step 262, loss: 25825.332031\n",
      "step 263, loss: 25763.673828\n",
      "step 264, loss: 25701.935547\n",
      "step 265, loss: 25640.511719\n",
      "step 266, loss: 25579.148438\n",
      "step 267, loss: 25517.867188\n",
      "step 268, loss: 25456.816406\n",
      "step 269, loss: 25395.835938\n",
      "step 270, loss: 25335.037109\n",
      "step 271, loss: 25274.371094\n",
      "step 272, loss: 25213.867188\n",
      "step 273, loss: 25153.457031\n",
      "step 274, loss: 25093.195312\n",
      "step 275, loss: 25033.093750\n",
      "step 276, loss: 24973.117188\n",
      "step 277, loss: 24913.193359\n",
      "step 278, loss: 24853.453125\n",
      "step 279, loss: 24793.873047\n",
      "step 280, loss: 24734.408203\n",
      "step 281, loss: 24674.960938\n",
      "step 282, loss: 24615.818359\n",
      "step 283, loss: 24556.718750\n",
      "step 284, loss: 24497.845703\n",
      "step 285, loss: 24438.974609\n",
      "step 286, loss: 24380.255859\n",
      "step 287, loss: 24321.808594\n",
      "step 288, loss: 24263.361328\n",
      "step 289, loss: 24205.140625\n",
      "step 290, loss: 24147.003906\n",
      "step 291, loss: 24088.859375\n",
      "step 292, loss: 24030.945312\n",
      "step 293, loss: 23973.279297\n",
      "step 294, loss: 23915.656250\n",
      "step 295, loss: 23858.128906\n",
      "step 296, loss: 23800.750000\n",
      "step 297, loss: 23743.578125\n",
      "step 298, loss: 23686.449219\n",
      "step 299, loss: 23629.326172\n",
      "step 300, loss: 23572.521484\n",
      "step 301, loss: 23515.845703\n",
      "step 302, loss: 23459.234375\n",
      "step 303, loss: 23402.769531\n",
      "step 304, loss: 23346.384766\n",
      "step 305, loss: 23290.232422\n",
      "step 306, loss: 23234.089844\n",
      "step 307, loss: 23178.179688\n",
      "step 308, loss: 23122.347656\n",
      "step 309, loss: 23066.630859\n",
      "step 310, loss: 23011.056641\n",
      "step 311, loss: 22955.630859\n",
      "step 312, loss: 22900.257812\n",
      "step 313, loss: 22845.107422\n",
      "step 314, loss: 22789.992188\n",
      "step 315, loss: 22735.052734\n",
      "step 316, loss: 22680.246094\n",
      "step 317, loss: 22625.558594\n",
      "step 318, loss: 22570.951172\n",
      "step 319, loss: 22516.500000\n",
      "step 320, loss: 22462.216797\n",
      "step 321, loss: 22408.011719\n",
      "step 322, loss: 22353.875000\n",
      "step 323, loss: 22299.980469\n",
      "step 324, loss: 22246.109375\n",
      "step 325, loss: 22192.433594\n",
      "step 326, loss: 22138.820312\n",
      "step 327, loss: 22085.324219\n",
      "step 328, loss: 22032.009766\n",
      "step 329, loss: 21978.806641\n",
      "step 330, loss: 21925.695312\n",
      "step 331, loss: 21872.791016\n",
      "step 332, loss: 21819.896484\n",
      "step 333, loss: 21767.220703\n",
      "step 334, loss: 21714.626953\n",
      "step 335, loss: 21662.162109\n",
      "step 336, loss: 21609.820312\n",
      "step 337, loss: 21557.503906\n",
      "step 338, loss: 21505.408203\n",
      "step 339, loss: 21453.414062\n",
      "step 340, loss: 21401.511719\n",
      "step 341, loss: 21349.781250\n",
      "step 342, loss: 21298.140625\n",
      "step 343, loss: 21246.716797\n",
      "step 344, loss: 21195.248047\n",
      "step 345, loss: 21143.960938\n",
      "step 346, loss: 21092.843750\n",
      "step 347, loss: 21041.767578\n",
      "step 348, loss: 20990.894531\n",
      "step 349, loss: 20940.035156\n",
      "step 350, loss: 20889.453125\n",
      "step 351, loss: 20838.810547\n",
      "step 352, loss: 20788.402344\n",
      "step 353, loss: 20738.103516\n",
      "step 354, loss: 20687.851562\n",
      "step 355, loss: 20637.771484\n",
      "step 356, loss: 20587.820312\n",
      "step 357, loss: 20537.941406\n",
      "step 358, loss: 20488.220703\n",
      "step 359, loss: 20438.595703\n",
      "step 360, loss: 20389.046875\n",
      "step 361, loss: 20339.697266\n",
      "step 362, loss: 20290.390625\n",
      "step 363, loss: 20241.275391\n",
      "step 364, loss: 20192.195312\n",
      "step 365, loss: 20143.277344\n",
      "step 366, loss: 20094.449219\n",
      "step 367, loss: 20045.765625\n",
      "step 368, loss: 19997.242188\n",
      "step 369, loss: 19948.740234\n",
      "step 370, loss: 19900.378906\n",
      "step 371, loss: 19852.164062\n",
      "step 372, loss: 19804.064453\n",
      "step 373, loss: 19756.019531\n",
      "step 374, loss: 19708.146484\n",
      "step 375, loss: 19660.390625\n",
      "step 376, loss: 19612.646484\n",
      "step 377, loss: 19565.181641\n",
      "step 378, loss: 19517.732422\n",
      "step 379, loss: 19470.369141\n",
      "step 380, loss: 19423.207031\n",
      "step 381, loss: 19376.039062\n",
      "step 382, loss: 19329.021484\n",
      "step 383, loss: 19282.150391\n",
      "step 384, loss: 19235.378906\n",
      "step 385, loss: 19188.761719\n",
      "step 386, loss: 19142.189453\n",
      "step 387, loss: 19095.755859\n",
      "step 388, loss: 19049.464844\n",
      "step 389, loss: 19003.212891\n",
      "step 390, loss: 18957.148438\n",
      "step 391, loss: 18911.160156\n",
      "step 392, loss: 18865.332031\n",
      "step 393, loss: 18819.500000\n",
      "step 394, loss: 18773.843750\n",
      "step 395, loss: 18728.281250\n",
      "step 396, loss: 18682.863281\n",
      "step 397, loss: 18637.558594\n",
      "step 398, loss: 18592.277344\n",
      "step 399, loss: 18547.175781\n",
      "step 400, loss: 18502.152344\n",
      "step 401, loss: 18457.328125\n",
      "step 402, loss: 18412.578125\n",
      "step 403, loss: 18367.822266\n",
      "step 404, loss: 18323.265625\n",
      "step 405, loss: 18278.753906\n",
      "step 406, loss: 18234.443359\n",
      "step 407, loss: 18190.195312\n",
      "step 408, loss: 18145.992188\n",
      "step 409, loss: 18102.007812\n",
      "step 410, loss: 18058.089844\n",
      "step 411, loss: 18014.269531\n",
      "step 412, loss: 17970.539062\n",
      "step 413, loss: 17926.937500\n",
      "step 414, loss: 17883.480469\n",
      "step 415, loss: 17840.066406\n",
      "step 416, loss: 17796.724609\n",
      "step 417, loss: 17753.593750\n",
      "step 418, loss: 17710.468750\n",
      "step 419, loss: 17667.511719\n",
      "step 420, loss: 17624.611328\n",
      "step 421, loss: 17581.902344\n",
      "step 422, loss: 17539.226562\n",
      "step 423, loss: 17496.642578\n",
      "step 424, loss: 17454.218750\n",
      "step 425, loss: 17411.857422\n",
      "step 426, loss: 17369.605469\n",
      "step 427, loss: 17327.468750\n",
      "step 428, loss: 17285.417969\n",
      "step 429, loss: 17243.527344\n",
      "step 430, loss: 17201.679688\n",
      "step 431, loss: 17159.960938\n",
      "step 432, loss: 17118.285156\n",
      "step 433, loss: 17076.771484\n",
      "step 434, loss: 17035.359375\n",
      "step 435, loss: 16994.023438\n",
      "step 436, loss: 16952.785156\n",
      "step 437, loss: 16911.695312\n",
      "step 438, loss: 16870.636719\n",
      "step 439, loss: 16829.792969\n",
      "step 440, loss: 16788.888672\n",
      "step 441, loss: 16748.244141\n",
      "step 442, loss: 16707.656250\n",
      "step 443, loss: 16667.117188\n",
      "step 444, loss: 16626.705078\n",
      "step 445, loss: 16586.429688\n",
      "step 446, loss: 16546.179688\n",
      "step 447, loss: 16506.054688\n",
      "step 448, loss: 16466.097656\n",
      "step 449, loss: 16426.142578\n",
      "step 450, loss: 16386.382812\n",
      "step 451, loss: 16346.662109\n",
      "step 452, loss: 16307.002930\n",
      "step 453, loss: 16267.505859\n",
      "step 454, loss: 16228.066406\n",
      "step 455, loss: 16188.792969\n",
      "step 456, loss: 16149.527344\n",
      "step 457, loss: 16110.419922\n",
      "step 458, loss: 16071.423828\n",
      "step 459, loss: 16032.474609\n",
      "step 460, loss: 15993.686523\n",
      "step 461, loss: 15954.914062\n",
      "step 462, loss: 15916.311523\n",
      "step 463, loss: 15877.718750\n",
      "step 464, loss: 15839.318359\n",
      "step 465, loss: 15800.974609\n",
      "step 466, loss: 15762.708984\n",
      "step 467, loss: 15724.578125\n",
      "step 468, loss: 15686.497070\n",
      "step 469, loss: 15648.531250\n",
      "step 470, loss: 15610.690430\n",
      "step 471, loss: 15572.932617\n",
      "step 472, loss: 15535.250977\n",
      "step 473, loss: 15497.650391\n",
      "step 474, loss: 15460.198242\n",
      "step 475, loss: 15422.781250\n",
      "step 476, loss: 15385.459961\n",
      "step 477, loss: 15348.293945\n",
      "step 478, loss: 15311.164062\n",
      "step 479, loss: 15274.179688\n",
      "step 480, loss: 15237.274414\n",
      "step 481, loss: 15200.431641\n",
      "step 482, loss: 15163.716797\n",
      "step 483, loss: 15127.019531\n",
      "step 484, loss: 15090.498047\n",
      "step 485, loss: 15054.074219\n",
      "step 486, loss: 15017.689453\n",
      "step 487, loss: 14981.422852\n",
      "step 488, loss: 14945.266602\n",
      "step 489, loss: 14909.173828\n",
      "step 490, loss: 14873.176758\n",
      "step 491, loss: 14837.316406\n",
      "step 492, loss: 14801.466797\n",
      "step 493, loss: 14765.789062\n",
      "step 494, loss: 14730.185547\n",
      "step 495, loss: 14694.638672\n",
      "step 496, loss: 14659.193359\n",
      "step 497, loss: 14623.853516\n",
      "step 498, loss: 14588.578125\n",
      "step 499, loss: 14553.401367\n",
      "step 500, loss: 14518.314453\n",
      "step 501, loss: 14483.337891\n",
      "step 502, loss: 14448.442383\n",
      "step 503, loss: 14413.623047\n",
      "step 504, loss: 14378.876953\n",
      "step 505, loss: 14344.257812\n",
      "step 506, loss: 14309.750000\n",
      "step 507, loss: 14275.242188\n",
      "step 508, loss: 14240.943359\n",
      "step 509, loss: 14206.636719\n",
      "step 510, loss: 14172.401367\n",
      "step 511, loss: 14138.340820\n",
      "step 512, loss: 14104.317383\n",
      "step 513, loss: 14070.445312\n",
      "step 514, loss: 14036.585938\n",
      "step 515, loss: 14002.821289\n",
      "step 516, loss: 13969.146484\n",
      "step 517, loss: 13935.559570\n",
      "step 518, loss: 13902.119141\n",
      "step 519, loss: 13868.730469\n",
      "step 520, loss: 13835.411133\n",
      "step 521, loss: 13802.167969\n",
      "step 522, loss: 13769.035156\n",
      "step 523, loss: 13735.988281\n",
      "step 524, loss: 13703.019531\n",
      "step 525, loss: 13670.156250\n",
      "step 526, loss: 13637.382812\n",
      "step 527, loss: 13604.613281\n",
      "step 528, loss: 13572.017578\n",
      "step 529, loss: 13539.455078\n",
      "step 530, loss: 13507.021484\n",
      "step 531, loss: 13474.669922\n",
      "step 532, loss: 13442.410156\n",
      "step 533, loss: 13410.198242\n",
      "step 534, loss: 13378.046875\n",
      "step 535, loss: 13346.060547\n",
      "step 536, loss: 13314.097656\n",
      "step 537, loss: 13282.216797\n",
      "step 538, loss: 13250.456055\n",
      "step 539, loss: 13218.738281\n",
      "step 540, loss: 13187.162109\n",
      "step 541, loss: 13155.595703\n",
      "step 542, loss: 13124.173828\n",
      "step 543, loss: 13092.773438\n",
      "step 544, loss: 13061.554688\n",
      "step 545, loss: 13030.332031\n",
      "step 546, loss: 12999.181641\n",
      "step 547, loss: 12968.186523\n",
      "step 548, loss: 12937.240234\n",
      "step 549, loss: 12906.365234\n",
      "step 550, loss: 12875.533203\n",
      "step 551, loss: 12844.862305\n",
      "step 552, loss: 12814.205078\n",
      "step 553, loss: 12783.671875\n",
      "step 554, loss: 12753.232422\n",
      "step 555, loss: 12722.899414\n",
      "step 556, loss: 12692.568359\n",
      "step 557, loss: 12662.378906\n",
      "step 558, loss: 12632.223633\n",
      "step 559, loss: 12602.128906\n",
      "step 560, loss: 12572.152344\n",
      "step 561, loss: 12542.279297\n",
      "step 562, loss: 12512.454102\n",
      "step 563, loss: 12482.732422\n",
      "step 564, loss: 12453.018555\n",
      "step 565, loss: 12423.494141\n",
      "step 566, loss: 12393.980469\n",
      "step 567, loss: 12364.533203\n",
      "step 568, loss: 12335.208984\n",
      "step 569, loss: 12305.959961\n",
      "step 570, loss: 12276.751953\n",
      "step 571, loss: 12247.658203\n",
      "step 572, loss: 12218.616211\n",
      "step 573, loss: 12189.636719\n",
      "step 574, loss: 12160.792969\n",
      "step 575, loss: 12132.016602\n",
      "step 576, loss: 12103.277344\n",
      "step 577, loss: 12074.666992\n",
      "step 578, loss: 12046.083984\n",
      "step 579, loss: 12017.610352\n",
      "step 580, loss: 11989.160156\n",
      "step 581, loss: 11960.853516\n",
      "step 582, loss: 11932.615234\n",
      "step 583, loss: 11904.414062\n",
      "step 584, loss: 11876.291016\n",
      "step 585, loss: 11848.252930\n",
      "step 586, loss: 11820.313477\n",
      "step 587, loss: 11792.439453\n",
      "step 588, loss: 11764.628906\n",
      "step 589, loss: 11736.910156\n",
      "step 590, loss: 11709.267578\n",
      "step 591, loss: 11681.712891\n",
      "step 592, loss: 11654.184570\n",
      "step 593, loss: 11626.756836\n",
      "step 594, loss: 11599.413086\n",
      "step 595, loss: 11572.123047\n",
      "step 596, loss: 11544.927734\n",
      "step 597, loss: 11517.833984\n",
      "step 598, loss: 11490.748047\n",
      "step 599, loss: 11463.759766\n",
      "step 600, loss: 11436.884766\n",
      "step 601, loss: 11410.036133\n",
      "step 602, loss: 11383.310547\n",
      "step 603, loss: 11356.560547\n",
      "step 604, loss: 11329.964844\n",
      "step 605, loss: 11303.452148\n",
      "step 606, loss: 11276.945312\n",
      "step 607, loss: 11250.573242\n",
      "step 608, loss: 11224.212891\n",
      "step 609, loss: 11197.983398\n",
      "step 610, loss: 11171.813477\n",
      "step 611, loss: 11145.681641\n",
      "step 612, loss: 11119.661133\n",
      "step 613, loss: 11093.703125\n",
      "step 614, loss: 11067.813477\n",
      "step 615, loss: 11041.994141\n",
      "step 616, loss: 11016.232422\n",
      "step 617, loss: 10990.583008\n",
      "step 618, loss: 10964.936523\n",
      "step 619, loss: 10939.423828\n",
      "step 620, loss: 10913.958008\n",
      "step 621, loss: 10888.535156\n",
      "step 622, loss: 10863.212891\n",
      "step 623, loss: 10837.972656\n",
      "step 624, loss: 10812.804688\n",
      "step 625, loss: 10787.668945\n",
      "step 626, loss: 10762.625977\n",
      "step 627, loss: 10737.636719\n",
      "step 628, loss: 10712.739258\n",
      "step 629, loss: 10687.921875\n",
      "step 630, loss: 10663.144531\n",
      "step 631, loss: 10638.425781\n",
      "step 632, loss: 10613.783203\n",
      "step 633, loss: 10589.248047\n",
      "step 634, loss: 10564.768555\n",
      "step 635, loss: 10540.320312\n",
      "step 636, loss: 10515.997070\n",
      "step 637, loss: 10491.691406\n",
      "step 638, loss: 10467.458984\n",
      "step 639, loss: 10443.302734\n",
      "step 640, loss: 10419.240234\n",
      "step 641, loss: 10395.168945\n",
      "step 642, loss: 10371.238281\n",
      "step 643, loss: 10347.385742\n",
      "step 644, loss: 10323.583008\n",
      "step 645, loss: 10299.831055\n",
      "step 646, loss: 10276.121094\n",
      "step 647, loss: 10252.511719\n",
      "step 648, loss: 10228.958984\n",
      "step 649, loss: 10205.472656\n",
      "step 650, loss: 10182.070312\n",
      "step 651, loss: 10158.717773\n",
      "step 652, loss: 10135.421875\n",
      "step 653, loss: 10112.201172\n",
      "step 654, loss: 10089.058594\n",
      "step 655, loss: 10065.957031\n",
      "step 656, loss: 10042.952148\n",
      "step 657, loss: 10019.992188\n",
      "step 658, loss: 9997.083984\n",
      "step 659, loss: 9974.252930\n",
      "step 660, loss: 9951.500000\n",
      "step 661, loss: 9928.791016\n",
      "step 662, loss: 9906.187500\n",
      "step 663, loss: 9883.595703\n",
      "step 664, loss: 9861.096680\n",
      "step 665, loss: 9838.637695\n",
      "step 666, loss: 9816.260742\n",
      "step 667, loss: 9793.952148\n",
      "step 668, loss: 9771.699219\n",
      "step 669, loss: 9749.525391\n",
      "step 670, loss: 9727.392578\n",
      "step 671, loss: 9705.337891\n",
      "step 672, loss: 9683.312500\n",
      "step 673, loss: 9661.410156\n",
      "step 674, loss: 9639.546875\n",
      "step 675, loss: 9617.708984\n",
      "step 676, loss: 9595.943359\n",
      "step 677, loss: 9574.265625\n",
      "step 678, loss: 9552.640625\n",
      "step 679, loss: 9531.108398\n",
      "step 680, loss: 9509.570312\n",
      "step 681, loss: 9488.163086\n",
      "step 682, loss: 9466.762695\n",
      "step 683, loss: 9445.458008\n",
      "step 684, loss: 9424.193359\n",
      "step 685, loss: 9403.038086\n",
      "step 686, loss: 9381.876953\n",
      "step 687, loss: 9360.822266\n",
      "step 688, loss: 9339.835938\n",
      "step 689, loss: 9318.847656\n",
      "step 690, loss: 9297.976562\n",
      "step 691, loss: 9277.132812\n",
      "step 692, loss: 9256.390625\n",
      "step 693, loss: 9235.664062\n",
      "step 694, loss: 9215.018555\n",
      "step 695, loss: 9194.447266\n",
      "step 696, loss: 9173.869141\n",
      "step 697, loss: 9153.423828\n",
      "step 698, loss: 9133.050781\n",
      "step 699, loss: 9112.688477\n",
      "step 700, loss: 9092.390625\n",
      "step 701, loss: 9072.148438\n",
      "step 702, loss: 9051.964844\n",
      "step 703, loss: 9031.823242\n",
      "step 704, loss: 9011.817383\n",
      "step 705, loss: 8991.814453\n",
      "step 706, loss: 8971.866211\n",
      "step 707, loss: 8951.976562\n",
      "step 708, loss: 8932.146484\n",
      "step 709, loss: 8912.405273\n",
      "step 710, loss: 8892.689453\n",
      "step 711, loss: 8873.038086\n",
      "step 712, loss: 8853.453125\n",
      "step 713, loss: 8833.925781\n",
      "step 714, loss: 8814.422852\n",
      "step 715, loss: 8795.028320\n",
      "step 716, loss: 8775.631836\n",
      "step 717, loss: 8756.347656\n",
      "step 718, loss: 8737.092773\n",
      "step 719, loss: 8717.900391\n",
      "step 720, loss: 8698.771484\n",
      "step 721, loss: 8679.698242\n",
      "step 722, loss: 8660.648438\n",
      "step 723, loss: 8641.696289\n",
      "step 724, loss: 8622.796875\n",
      "step 725, loss: 8603.932617\n",
      "step 726, loss: 8585.123047\n",
      "step 727, loss: 8566.382812\n",
      "step 728, loss: 8547.679688\n",
      "step 729, loss: 8529.050781\n",
      "step 730, loss: 8510.481445\n",
      "step 731, loss: 8491.936523\n",
      "step 732, loss: 8473.463867\n",
      "step 733, loss: 8455.070312\n",
      "step 734, loss: 8436.710938\n",
      "step 735, loss: 8418.390625\n",
      "step 736, loss: 8400.145508\n",
      "step 737, loss: 8381.935547\n",
      "step 738, loss: 8363.793945\n",
      "step 739, loss: 8345.721680\n",
      "step 740, loss: 8327.682617\n",
      "step 741, loss: 8309.703125\n",
      "step 742, loss: 8291.785156\n",
      "step 743, loss: 8273.884766\n",
      "step 744, loss: 8256.074219\n",
      "step 745, loss: 8238.296875\n",
      "step 746, loss: 8220.579102\n",
      "step 747, loss: 8202.933594\n",
      "step 748, loss: 8185.323242\n",
      "step 749, loss: 8167.772461\n",
      "step 750, loss: 8150.243164\n",
      "step 751, loss: 8132.805176\n",
      "step 752, loss: 8115.414062\n",
      "step 753, loss: 8098.062500\n",
      "step 754, loss: 8080.765625\n",
      "step 755, loss: 8063.530273\n",
      "step 756, loss: 8046.321289\n",
      "step 757, loss: 8029.180664\n",
      "step 758, loss: 8012.102539\n",
      "step 759, loss: 7995.068359\n",
      "step 760, loss: 7978.080078\n",
      "step 761, loss: 7961.148438\n",
      "step 762, loss: 7944.268555\n",
      "step 763, loss: 7927.429688\n",
      "step 764, loss: 7910.648438\n",
      "step 765, loss: 7893.906250\n",
      "step 766, loss: 7877.251953\n",
      "step 767, loss: 7860.613281\n",
      "step 768, loss: 7844.034180\n",
      "step 769, loss: 7827.495117\n",
      "step 770, loss: 7811.000977\n",
      "step 771, loss: 7794.616699\n",
      "step 772, loss: 7778.214844\n",
      "step 773, loss: 7761.898438\n",
      "step 774, loss: 7745.619141\n",
      "step 775, loss: 7729.375977\n",
      "step 776, loss: 7713.187500\n",
      "step 777, loss: 7697.073242\n",
      "step 778, loss: 7680.993164\n",
      "step 779, loss: 7664.954102\n",
      "step 780, loss: 7648.966797\n",
      "step 781, loss: 7633.044922\n",
      "step 782, loss: 7617.152344\n",
      "step 783, loss: 7601.294922\n",
      "step 784, loss: 7585.525391\n",
      "step 785, loss: 7569.785156\n",
      "step 786, loss: 7554.071289\n",
      "step 787, loss: 7538.448730\n",
      "step 788, loss: 7522.845703\n",
      "step 789, loss: 7507.291016\n",
      "step 790, loss: 7491.789062\n",
      "step 791, loss: 7476.327148\n",
      "step 792, loss: 7460.922363\n",
      "step 793, loss: 7445.570801\n",
      "step 794, loss: 7430.253906\n",
      "step 795, loss: 7415.002930\n",
      "step 796, loss: 7399.780273\n",
      "step 797, loss: 7384.608887\n",
      "step 798, loss: 7369.484375\n",
      "step 799, loss: 7354.400391\n",
      "step 800, loss: 7339.372070\n",
      "step 801, loss: 7324.396973\n",
      "step 802, loss: 7309.455566\n",
      "step 803, loss: 7294.559082\n",
      "step 804, loss: 7279.713379\n",
      "step 805, loss: 7264.916504\n",
      "step 806, loss: 7250.152344\n",
      "step 807, loss: 7235.440918\n",
      "step 808, loss: 7220.800781\n",
      "step 809, loss: 7206.170410\n",
      "step 810, loss: 7191.581055\n",
      "step 811, loss: 7177.082031\n",
      "step 812, loss: 7162.585938\n",
      "step 813, loss: 7148.155762\n",
      "step 814, loss: 7133.772461\n",
      "step 815, loss: 7119.424805\n",
      "step 816, loss: 7105.132324\n",
      "step 817, loss: 7090.880859\n",
      "step 818, loss: 7076.659668\n",
      "step 819, loss: 7062.463379\n",
      "step 820, loss: 7048.363281\n",
      "step 821, loss: 7034.280273\n",
      "step 822, loss: 7020.242188\n",
      "step 823, loss: 7006.246094\n",
      "step 824, loss: 6992.297852\n",
      "step 825, loss: 6978.395020\n",
      "step 826, loss: 6964.549805\n",
      "step 827, loss: 6950.710938\n",
      "step 828, loss: 6936.947754\n",
      "step 829, loss: 6923.218750\n",
      "step 830, loss: 6909.531250\n",
      "step 831, loss: 6895.886230\n",
      "step 832, loss: 6882.277344\n",
      "step 833, loss: 6868.720703\n",
      "step 834, loss: 6855.210938\n",
      "step 835, loss: 6841.750000\n",
      "step 836, loss: 6828.310059\n",
      "step 837, loss: 6814.910645\n",
      "step 838, loss: 6801.558594\n",
      "step 839, loss: 6788.259277\n",
      "step 840, loss: 6775.009277\n",
      "step 841, loss: 6761.801758\n",
      "step 842, loss: 6748.611816\n",
      "step 843, loss: 6735.462891\n",
      "step 844, loss: 6722.363281\n",
      "step 845, loss: 6709.328125\n",
      "step 846, loss: 6696.316895\n",
      "step 847, loss: 6683.335938\n",
      "step 848, loss: 6670.413086\n",
      "step 849, loss: 6657.521973\n",
      "step 850, loss: 6644.692383\n",
      "step 851, loss: 6631.869141\n",
      "step 852, loss: 6619.112305\n",
      "step 853, loss: 6606.378418\n",
      "step 854, loss: 6593.682129\n",
      "step 855, loss: 6581.056152\n",
      "step 856, loss: 6568.431641\n",
      "step 857, loss: 6555.865234\n",
      "step 858, loss: 6543.338867\n",
      "step 859, loss: 6530.870605\n",
      "step 860, loss: 6518.443359\n",
      "step 861, loss: 6506.026855\n",
      "step 862, loss: 6493.662109\n",
      "step 863, loss: 6481.312500\n",
      "step 864, loss: 6469.045898\n",
      "step 865, loss: 6456.798340\n",
      "step 866, loss: 6444.598633\n",
      "step 867, loss: 6432.434570\n",
      "step 868, loss: 6420.285156\n",
      "step 869, loss: 6408.208984\n",
      "step 870, loss: 6396.150391\n",
      "step 871, loss: 6384.152344\n",
      "step 872, loss: 6372.179688\n",
      "step 873, loss: 6360.244629\n",
      "step 874, loss: 6348.344727\n",
      "step 875, loss: 6336.483398\n",
      "step 876, loss: 6324.672852\n",
      "step 877, loss: 6312.890625\n",
      "step 878, loss: 6301.147949\n",
      "step 879, loss: 6289.434570\n",
      "step 880, loss: 6277.790039\n",
      "step 881, loss: 6266.168945\n",
      "step 882, loss: 6254.556152\n",
      "step 883, loss: 6242.999512\n",
      "step 884, loss: 6231.498047\n",
      "step 885, loss: 6220.013184\n",
      "step 886, loss: 6208.556641\n",
      "step 887, loss: 6197.160156\n",
      "step 888, loss: 6185.785156\n",
      "step 889, loss: 6174.469727\n",
      "step 890, loss: 6163.166992\n",
      "step 891, loss: 6151.923828\n",
      "step 892, loss: 6140.699707\n",
      "step 893, loss: 6129.515625\n",
      "step 894, loss: 6118.341797\n",
      "step 895, loss: 6107.234863\n",
      "step 896, loss: 6096.170898\n",
      "step 897, loss: 6085.133789\n",
      "step 898, loss: 6074.143555\n",
      "step 899, loss: 6063.166992\n",
      "step 900, loss: 6052.246094\n",
      "step 901, loss: 6041.345703\n",
      "step 902, loss: 6030.500000\n",
      "step 903, loss: 6019.661133\n",
      "step 904, loss: 6008.882324\n",
      "step 905, loss: 5998.105957\n",
      "step 906, loss: 5987.418945\n",
      "step 907, loss: 5976.726562\n",
      "step 908, loss: 5966.049805\n",
      "step 909, loss: 5955.458008\n",
      "step 910, loss: 5944.885742\n",
      "step 911, loss: 5934.330078\n",
      "step 912, loss: 5923.831055\n",
      "step 913, loss: 5913.351074\n",
      "step 914, loss: 5902.909180\n",
      "step 915, loss: 5892.505371\n",
      "step 916, loss: 5882.134766\n",
      "step 917, loss: 5871.828125\n",
      "step 918, loss: 5861.511719\n",
      "step 919, loss: 5851.250000\n",
      "step 920, loss: 5841.012207\n",
      "step 921, loss: 5830.806641\n",
      "step 922, loss: 5820.650391\n",
      "step 923, loss: 5810.504883\n",
      "step 924, loss: 5800.416992\n",
      "step 925, loss: 5790.333496\n",
      "step 926, loss: 5780.311523\n",
      "step 927, loss: 5770.315918\n",
      "step 928, loss: 5760.352539\n",
      "step 929, loss: 5750.417969\n",
      "step 930, loss: 5740.519531\n",
      "step 931, loss: 5730.653320\n",
      "step 932, loss: 5720.805664\n",
      "step 933, loss: 5711.008789\n",
      "step 934, loss: 5701.238281\n",
      "step 935, loss: 5691.528320\n",
      "step 936, loss: 5681.804199\n",
      "step 937, loss: 5672.134766\n",
      "step 938, loss: 5662.520508\n",
      "step 939, loss: 5652.895996\n",
      "step 940, loss: 5643.326172\n",
      "step 941, loss: 5633.788574\n",
      "step 942, loss: 5624.269043\n",
      "step 943, loss: 5614.793945\n",
      "step 944, loss: 5605.357422\n",
      "step 945, loss: 5595.920410\n",
      "step 946, loss: 5586.549316\n",
      "step 947, loss: 5577.194336\n",
      "step 948, loss: 5567.877441\n",
      "step 949, loss: 5558.593750\n",
      "step 950, loss: 5549.343750\n",
      "step 951, loss: 5540.096680\n",
      "step 952, loss: 5530.907227\n",
      "step 953, loss: 5521.750488\n",
      "step 954, loss: 5512.620117\n",
      "step 955, loss: 5503.512695\n",
      "step 956, loss: 5494.430664\n",
      "step 957, loss: 5485.389648\n",
      "step 958, loss: 5476.382812\n",
      "step 959, loss: 5467.399414\n",
      "step 960, loss: 5458.448242\n",
      "step 961, loss: 5449.547363\n",
      "step 962, loss: 5440.625000\n",
      "step 963, loss: 5431.783691\n",
      "step 964, loss: 5422.948242\n",
      "step 965, loss: 5414.145508\n",
      "step 966, loss: 5405.380371\n",
      "step 967, loss: 5396.641602\n",
      "step 968, loss: 5387.930664\n",
      "step 969, loss: 5379.232910\n",
      "step 970, loss: 5370.592285\n",
      "step 971, loss: 5361.972656\n",
      "step 972, loss: 5353.375000\n",
      "step 973, loss: 5344.808594\n",
      "step 974, loss: 5336.278809\n",
      "step 975, loss: 5327.771973\n",
      "step 976, loss: 5319.284180\n",
      "step 977, loss: 5310.854004\n",
      "step 978, loss: 5302.419922\n",
      "step 979, loss: 5294.023438\n",
      "step 980, loss: 5285.674316\n",
      "step 981, loss: 5277.342773\n",
      "step 982, loss: 5269.046875\n",
      "step 983, loss: 5260.774902\n",
      "step 984, loss: 5252.524902\n",
      "step 985, loss: 5244.281250\n",
      "step 986, loss: 5236.093262\n",
      "step 987, loss: 5227.920898\n",
      "step 988, loss: 5219.797363\n",
      "step 989, loss: 5211.689453\n",
      "step 990, loss: 5203.605469\n",
      "step 991, loss: 5195.555664\n",
      "step 992, loss: 5187.533691\n",
      "step 993, loss: 5179.542969\n",
      "step 994, loss: 5171.558594\n",
      "step 995, loss: 5163.641602\n",
      "step 996, loss: 5155.720703\n",
      "step 997, loss: 5147.807617\n",
      "step 998, loss: 5139.958496\n",
      "step 999, loss: 5132.120605\n",
      "Pearson R^2: 0.511869\n",
      "RMS: 0.382109\n"
     ]
    }
   ],
   "source": [
    "n_steps = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Train model\n",
    "    for i in range(n_steps):\n",
    "        feed_dict = {x: x_np, y: y_np}\n",
    "        _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "        print(\"step %d, loss: %f\" % (i, loss))\n",
    "        train_writer.add_summary(summary, i)\n",
    "        \n",
    "    # Get weights\n",
    "    w_final, b_final = sess.run([W, b])\n",
    "\n",
    "    # Make Predictions\n",
    "    y_pred_np = sess.run(y_pred, feed_dict={x: x_np})\n",
    "        \n",
    "y_pred_np = np.reshape(y_pred_np, -1)\n",
    "r2 = pearson_r2_score(y_np, y_pred_np)\n",
    "print(\"Pearson R^2: %f\" % r2)\n",
    "rms = rms_score(y_np, y_pred_np)\n",
    "print(\"RMS: %f\" % rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9817898 , -0.36203694, -0.08542389,  0.40483868,  1.2386503 ,\n",
       "       -0.19740832, -0.11455667, -0.32873964,  0.1132696 , -0.6672922 ,\n",
       "        0.83919746,  0.62182444,  0.39941612, -0.37620026, -0.71736056,\n",
       "        0.7514019 ,  0.7175565 ,  0.7319519 , -0.53915364,  0.08564082,\n",
       "       -0.02143991, -0.02051699, -0.8579679 , -0.07014984,  0.1993329 ,\n",
       "        0.73754185,  0.1029312 , -0.01880336,  0.91389775, -0.07981193,\n",
       "       -0.2805807 , -0.10170358, -0.17609191,  0.5801829 , -0.41269803,\n",
       "        0.8740171 , -0.2279818 , -0.05835742, -0.21671015, -0.10408837,\n",
       "        0.6249766 ,  0.43739292,  0.3498876 , -0.31019515,  0.29029208,\n",
       "       -0.12522203, -0.20759064,  0.36398697, -0.27612305,  0.7152039 ,\n",
       "        0.57498175,  0.755962  ,  0.6982634 ,  1.0359428 ,  0.8758505 ,\n",
       "        0.7484901 ,  0.828894  ,  0.7591613 ,  0.9006707 ,  0.96355015,\n",
       "        0.82326484,  0.6669822 ,  0.7596264 ,  0.8777227 ,  0.7261989 ,\n",
       "        0.8859246 ,  1.0921686 ,  0.8528956 ,  0.790903  ,  0.91982466,\n",
       "        0.9169898 ,  1.0341241 ,  0.9469356 ,  1.0124278 ,  0.6795964 ,\n",
       "        0.82478637,  0.83292544,  0.85384846,  0.5448198 ,  0.732425  ,\n",
       "        1.0538306 ,  0.9457659 ,  0.69690835,  0.93597037,  0.99601686,\n",
       "        0.86417663,  0.7700949 ,  0.8964886 ,  0.7567992 ,  0.6633057 ,\n",
       "        0.80387676,  0.836869  ,  1.1081994 ,  0.71467113,  1.1753945 ,\n",
       "        1.0537815 ,  0.93634117,  1.007639  ,  0.69206744,  0.93907285],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"Y-true\")\n",
    "plt.ylabel(\"Y-pred\")\n",
    "plt.title(\"Predicted versus True values \"\n",
    "          r\"(Pearson $R^2$: $0.994$)\")\n",
    "plt.scatter(y_np, y_pred_np)\n",
    "plt.savefig(\"lr_pred.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VISUALIZING LINEAR REGRESSION MODELS WITH TENSORBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hakan/.pyenv/versions/3.6.4/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2018-08-15 17:53:26.554161: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[33mW0815 17:53:26.570664 Reloader tf_logging.py:121] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0815 17:53:26.571692 Reloader tf_logging.py:121] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0815 17:53:26.573658 Reloader tf_logging.py:121] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0815 17:53:26.574537 Reloader tf_logging.py:121] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0815 17:53:26.575924 Reloader tf_logging.py:121] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0815 17:53:26.576663 Reloader tf_logging.py:121] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0815 17:53:26.577956 Reloader tf_logging.py:121] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0815 17:53:26.578451 Reloader tf_logging.py:121] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0mTensorBoard 1.8.0 at http://hakans-planet:6006 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir=/tmp/lr-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Generate tensorflow graph\n",
    "with tf.name_scope('logistic'):\n",
    "    with tf.name_scope(\"placeholders\"):\n",
    "        # Note that our datapoints x are 2-dimensional.\n",
    "        x = tf.placeholder(tf.float32, (N, 2))\n",
    "        y = tf.placeholder(tf.float32, (N,))\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W = tf.Variable(tf.random_normal((2, 1)))\n",
    "        b = tf.Variable(tf.random_normal((1,)))\n",
    "    \n",
    "    with tf.name_scope(\"prediction\"):\n",
    "        y_logit = tf.squeeze(tf.matmul(x, W) + b)\n",
    "        # the sigmoid gives the class probability of 1\n",
    "        y_one_prob = tf.sigmoid(y_logit)\n",
    "        # Rounding P(y=1) will give the correct prediction.\n",
    "        y_pred = tf.round(y_one_prob)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # Compute the cross-entropy term for each datapoint\n",
    "        entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y)\n",
    "        # Sum all contributions\n",
    "        l = tf.reduce_sum(entropy)\n",
    "        \n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        tf.summary.scalar(\"loss\", l)\n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "    with tf.name_scope(\"optim\"):\n",
    "        train_op = tf.train.AdamOptimizer(.01).minimize(l)\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('/tmp/logistic-train', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 56.666477\n",
      "loss: 55.820042\n",
      "loss: 54.988415\n",
      "loss: 54.171757\n",
      "loss: 53.370213\n",
      "loss: 52.583870\n",
      "loss: 51.812801\n",
      "loss: 51.057037\n",
      "loss: 50.316555\n",
      "loss: 49.591297\n",
      "loss: 48.881157\n",
      "loss: 48.185986\n",
      "loss: 47.505581\n",
      "loss: 46.839706\n",
      "loss: 46.188084\n",
      "loss: 45.550388\n",
      "loss: 44.926281\n",
      "loss: 44.315403\n",
      "loss: 43.717373\n",
      "loss: 43.131821\n",
      "loss: 42.558372\n",
      "loss: 41.996681\n",
      "loss: 41.446419\n",
      "loss: 40.907276\n",
      "loss: 40.378979\n",
      "loss: 39.861271\n",
      "loss: 39.353935\n",
      "loss: 38.856773\n",
      "loss: 38.369591\n",
      "loss: 37.892239\n",
      "loss: 37.424541\n",
      "loss: 36.966362\n",
      "loss: 36.517551\n",
      "loss: 36.077961\n",
      "loss: 35.647449\n",
      "loss: 35.225872\n",
      "loss: 34.813065\n",
      "loss: 34.408871\n",
      "loss: 34.013126\n",
      "loss: 33.625671\n",
      "loss: 33.246319\n",
      "loss: 32.874908\n",
      "loss: 32.511250\n",
      "loss: 32.155163\n",
      "loss: 31.806469\n",
      "loss: 31.464981\n",
      "loss: 31.130516\n",
      "loss: 30.802896\n",
      "loss: 30.481947\n",
      "loss: 30.167490\n",
      "loss: 29.859356\n",
      "loss: 29.557377\n",
      "loss: 29.261391\n",
      "loss: 28.971243\n",
      "loss: 28.686773\n",
      "loss: 28.407839\n",
      "loss: 28.134295\n",
      "loss: 27.865995\n",
      "loss: 27.602814\n",
      "loss: 27.344620\n",
      "loss: 27.091278\n",
      "loss: 26.842672\n",
      "loss: 26.598680\n",
      "loss: 26.359184\n",
      "loss: 26.124073\n",
      "loss: 25.893234\n",
      "loss: 25.666563\n",
      "loss: 25.443954\n",
      "loss: 25.225304\n",
      "loss: 25.010517\n",
      "loss: 24.799492\n",
      "loss: 24.592136\n",
      "loss: 24.388361\n",
      "loss: 24.188074\n",
      "loss: 23.991192\n",
      "loss: 23.797628\n",
      "loss: 23.607296\n",
      "loss: 23.420126\n",
      "loss: 23.236038\n",
      "loss: 23.054955\n",
      "loss: 22.876806\n",
      "loss: 22.701521\n",
      "loss: 22.529032\n",
      "loss: 22.359270\n",
      "loss: 22.192177\n",
      "loss: 22.027687\n",
      "loss: 21.865742\n",
      "loss: 21.706280\n",
      "loss: 21.549246\n",
      "loss: 21.394587\n",
      "loss: 21.242250\n",
      "loss: 21.092178\n",
      "loss: 20.944326\n",
      "loss: 20.798643\n",
      "loss: 20.655085\n",
      "loss: 20.513599\n",
      "loss: 20.374146\n",
      "loss: 20.236679\n",
      "loss: 20.101156\n",
      "loss: 19.967539\n",
      "loss: 19.835785\n",
      "loss: 19.705854\n",
      "loss: 19.577713\n",
      "loss: 19.451321\n",
      "loss: 19.326639\n",
      "loss: 19.203638\n",
      "loss: 19.082283\n",
      "loss: 18.962538\n",
      "loss: 18.844376\n",
      "loss: 18.727757\n",
      "loss: 18.612656\n",
      "loss: 18.499044\n",
      "loss: 18.386890\n",
      "loss: 18.276165\n",
      "loss: 18.166843\n",
      "loss: 18.058897\n",
      "loss: 17.952301\n",
      "loss: 17.847029\n",
      "loss: 17.743059\n",
      "loss: 17.640362\n",
      "loss: 17.538918\n",
      "loss: 17.438698\n",
      "loss: 17.339689\n",
      "loss: 17.241859\n",
      "loss: 17.145193\n",
      "loss: 17.049669\n",
      "loss: 16.955267\n",
      "loss: 16.861967\n",
      "loss: 16.769749\n",
      "loss: 16.678593\n",
      "loss: 16.588482\n",
      "loss: 16.499395\n",
      "loss: 16.411320\n",
      "loss: 16.324234\n",
      "loss: 16.238123\n",
      "loss: 16.152969\n",
      "loss: 16.068756\n",
      "loss: 15.985472\n",
      "loss: 15.903097\n",
      "loss: 15.821616\n",
      "loss: 15.741016\n",
      "loss: 15.661284\n",
      "loss: 15.582405\n",
      "loss: 15.504363\n",
      "loss: 15.427146\n",
      "loss: 15.350742\n",
      "loss: 15.275136\n",
      "loss: 15.200317\n",
      "loss: 15.126271\n",
      "loss: 15.052987\n",
      "loss: 14.980454\n",
      "loss: 14.908659\n",
      "loss: 14.837591\n",
      "loss: 14.767239\n",
      "loss: 14.697590\n",
      "loss: 14.628636\n",
      "loss: 14.560368\n",
      "loss: 14.492772\n",
      "loss: 14.425840\n",
      "loss: 14.359562\n",
      "loss: 14.293927\n",
      "loss: 14.228928\n",
      "loss: 14.164555\n",
      "loss: 14.100797\n",
      "loss: 14.037645\n",
      "loss: 13.975095\n",
      "loss: 13.913133\n",
      "loss: 13.851752\n",
      "loss: 13.790946\n",
      "loss: 13.730703\n",
      "loss: 13.671020\n",
      "loss: 13.611887\n",
      "loss: 13.553295\n",
      "loss: 13.495236\n",
      "loss: 13.437706\n",
      "loss: 13.380697\n",
      "loss: 13.324199\n",
      "loss: 13.268207\n",
      "loss: 13.212713\n",
      "loss: 13.157714\n",
      "loss: 13.103198\n",
      "loss: 13.049163\n",
      "loss: 12.995600\n",
      "loss: 12.942504\n",
      "loss: 12.889869\n",
      "loss: 12.837688\n",
      "loss: 12.785956\n",
      "loss: 12.734667\n",
      "loss: 12.683815\n",
      "loss: 12.633395\n",
      "loss: 12.583400\n",
      "loss: 12.533827\n",
      "loss: 12.484668\n",
      "loss: 12.435919\n",
      "loss: 12.387574\n",
      "loss: 12.339631\n",
      "loss: 12.292082\n",
      "loss: 12.244923\n",
      "loss: 12.198148\n",
      "loss: 12.151754\n",
      "loss: 12.105736\n",
      "loss: 12.060087\n",
      "loss: 12.014808\n",
      "loss: 11.969890\n",
      "loss: 11.925328\n",
      "loss: 11.881122\n",
      "loss: 11.837265\n",
      "loss: 11.793750\n",
      "loss: 11.750580\n",
      "loss: 11.707746\n",
      "loss: 11.665244\n",
      "loss: 11.623071\n",
      "loss: 11.581225\n",
      "loss: 11.539699\n",
      "loss: 11.498490\n",
      "loss: 11.457597\n",
      "loss: 11.417016\n",
      "loss: 11.376740\n",
      "loss: 11.336767\n",
      "loss: 11.297097\n",
      "loss: 11.257723\n",
      "loss: 11.218640\n",
      "loss: 11.179849\n",
      "loss: 11.141343\n",
      "loss: 11.103123\n",
      "loss: 11.065182\n",
      "loss: 11.027520\n",
      "loss: 10.990131\n",
      "loss: 10.953014\n",
      "loss: 10.916165\n",
      "loss: 10.879581\n",
      "loss: 10.843261\n",
      "loss: 10.807200\n",
      "loss: 10.771396\n",
      "loss: 10.735847\n",
      "loss: 10.700551\n",
      "loss: 10.665501\n",
      "loss: 10.630698\n",
      "loss: 10.596138\n",
      "loss: 10.561821\n",
      "loss: 10.527740\n",
      "loss: 10.493897\n",
      "loss: 10.460288\n",
      "loss: 10.426909\n",
      "loss: 10.393759\n",
      "loss: 10.360836\n",
      "loss: 10.328135\n",
      "loss: 10.295659\n",
      "loss: 10.263402\n",
      "loss: 10.231359\n",
      "loss: 10.199534\n",
      "loss: 10.167922\n",
      "loss: 10.136520\n",
      "loss: 10.105328\n",
      "loss: 10.074341\n",
      "loss: 10.043561\n",
      "loss: 10.012980\n",
      "loss: 9.982602\n",
      "loss: 9.952422\n",
      "loss: 9.922439\n",
      "loss: 9.892651\n",
      "loss: 9.863054\n",
      "loss: 9.833650\n",
      "loss: 9.804434\n",
      "loss: 9.775407\n",
      "loss: 9.746564\n",
      "loss: 9.717905\n",
      "loss: 9.689427\n",
      "loss: 9.661129\n",
      "loss: 9.633011\n",
      "loss: 9.605069\n",
      "loss: 9.577303\n",
      "loss: 9.549710\n",
      "loss: 9.522290\n",
      "loss: 9.495037\n",
      "loss: 9.467957\n",
      "loss: 9.441042\n",
      "loss: 9.414291\n",
      "loss: 9.387705\n",
      "loss: 9.361282\n",
      "loss: 9.335020\n",
      "loss: 9.308918\n",
      "loss: 9.282974\n",
      "loss: 9.257186\n",
      "loss: 9.231552\n",
      "loss: 9.206074\n",
      "loss: 9.180748\n",
      "loss: 9.155573\n",
      "loss: 9.130548\n",
      "loss: 9.105671\n",
      "loss: 9.080941\n",
      "loss: 9.056356\n",
      "loss: 9.031916\n",
      "loss: 9.007620\n",
      "loss: 8.983465\n",
      "loss: 8.959452\n",
      "loss: 8.935577\n",
      "loss: 8.911841\n",
      "loss: 8.888242\n",
      "loss: 8.864779\n",
      "loss: 8.841450\n",
      "loss: 8.818254\n",
      "loss: 8.795192\n",
      "loss: 8.772262\n",
      "loss: 8.749460\n",
      "loss: 8.726789\n",
      "loss: 8.704245\n",
      "loss: 8.681828\n",
      "loss: 8.659535\n",
      "loss: 8.637368\n",
      "loss: 8.615324\n",
      "loss: 8.593405\n",
      "loss: 8.571605\n",
      "loss: 8.549927\n",
      "loss: 8.528368\n",
      "loss: 8.506927\n",
      "loss: 8.485605\n",
      "loss: 8.464399\n",
      "loss: 8.443309\n",
      "loss: 8.422333\n",
      "loss: 8.401471\n",
      "loss: 8.380723\n",
      "loss: 8.360085\n",
      "loss: 8.339560\n",
      "loss: 8.319144\n",
      "loss: 8.298838\n",
      "loss: 8.278642\n",
      "loss: 8.258551\n",
      "loss: 8.238565\n",
      "loss: 8.218688\n",
      "loss: 8.198915\n",
      "loss: 8.179246\n",
      "loss: 8.159681\n",
      "loss: 8.140219\n",
      "loss: 8.120857\n",
      "loss: 8.101597\n",
      "loss: 8.082437\n",
      "loss: 8.063376\n",
      "loss: 8.044414\n",
      "loss: 8.025550\n",
      "loss: 8.006783\n",
      "loss: 7.988111\n",
      "loss: 7.969535\n",
      "loss: 7.951055\n",
      "loss: 7.932668\n",
      "loss: 7.914375\n",
      "loss: 7.896173\n",
      "loss: 7.878066\n",
      "loss: 7.860048\n",
      "loss: 7.842121\n",
      "loss: 7.824284\n",
      "loss: 7.806537\n",
      "loss: 7.788877\n",
      "loss: 7.771307\n",
      "loss: 7.753822\n",
      "loss: 7.736424\n",
      "loss: 7.719112\n",
      "loss: 7.701886\n",
      "loss: 7.684745\n",
      "loss: 7.667686\n",
      "loss: 7.650712\n",
      "loss: 7.633820\n",
      "loss: 7.617011\n",
      "loss: 7.600283\n",
      "loss: 7.583636\n",
      "loss: 7.567068\n",
      "loss: 7.550582\n",
      "loss: 7.534174\n",
      "loss: 7.517846\n",
      "loss: 7.501595\n",
      "loss: 7.485422\n",
      "loss: 7.469326\n",
      "loss: 7.453306\n",
      "loss: 7.437363\n",
      "loss: 7.421494\n",
      "loss: 7.405699\n",
      "loss: 7.389980\n",
      "loss: 7.374334\n",
      "loss: 7.358763\n",
      "loss: 7.343262\n",
      "loss: 7.327835\n",
      "loss: 7.312479\n",
      "loss: 7.297194\n",
      "loss: 7.281980\n",
      "loss: 7.266836\n",
      "loss: 7.251761\n",
      "loss: 7.236756\n",
      "loss: 7.221819\n",
      "loss: 7.206952\n",
      "loss: 7.192152\n",
      "loss: 7.177419\n",
      "loss: 7.162753\n",
      "loss: 7.148153\n",
      "loss: 7.133619\n",
      "loss: 7.119151\n",
      "loss: 7.104748\n",
      "loss: 7.090409\n",
      "loss: 7.076136\n",
      "loss: 7.061924\n",
      "loss: 7.047778\n",
      "loss: 7.033693\n",
      "loss: 7.019672\n",
      "loss: 7.005712\n",
      "loss: 6.991814\n",
      "loss: 6.977978\n",
      "loss: 6.964200\n",
      "loss: 6.950485\n",
      "loss: 6.936830\n",
      "loss: 6.923233\n",
      "loss: 6.909696\n",
      "loss: 6.896219\n",
      "loss: 6.882800\n",
      "loss: 6.869439\n",
      "loss: 6.856136\n",
      "loss: 6.842889\n",
      "loss: 6.829699\n",
      "loss: 6.816567\n",
      "loss: 6.803490\n",
      "loss: 6.790470\n",
      "loss: 6.777504\n",
      "loss: 6.764595\n",
      "loss: 6.751739\n",
      "loss: 6.738939\n",
      "loss: 6.726192\n",
      "loss: 6.713499\n",
      "loss: 6.700860\n",
      "loss: 6.688272\n",
      "loss: 6.675739\n",
      "loss: 6.663257\n",
      "loss: 6.650828\n",
      "loss: 6.638451\n",
      "loss: 6.626124\n",
      "loss: 6.613849\n",
      "loss: 6.601625\n",
      "loss: 6.589451\n",
      "loss: 6.577327\n",
      "loss: 6.565253\n",
      "loss: 6.553228\n",
      "loss: 6.541253\n",
      "loss: 6.529326\n",
      "loss: 6.517448\n",
      "loss: 6.505618\n",
      "loss: 6.493837\n",
      "loss: 6.482102\n",
      "loss: 6.470416\n",
      "loss: 6.458777\n",
      "loss: 6.447184\n",
      "loss: 6.435638\n",
      "loss: 6.424138\n",
      "loss: 6.412685\n",
      "loss: 6.401276\n",
      "loss: 6.389914\n",
      "loss: 6.378596\n",
      "loss: 6.367324\n",
      "loss: 6.356097\n",
      "loss: 6.344912\n",
      "loss: 6.333772\n",
      "loss: 6.322677\n",
      "loss: 6.311624\n",
      "loss: 6.300615\n",
      "loss: 6.289650\n",
      "loss: 6.278726\n",
      "loss: 6.267846\n",
      "loss: 6.257007\n",
      "loss: 6.246211\n",
      "loss: 6.235456\n",
      "loss: 6.224743\n",
      "loss: 6.214072\n",
      "loss: 6.203441\n",
      "loss: 6.192851\n",
      "loss: 6.182301\n",
      "loss: 6.171792\n",
      "loss: 6.161323\n",
      "loss: 6.150895\n",
      "loss: 6.140504\n",
      "loss: 6.130155\n",
      "loss: 6.119843\n",
      "loss: 6.109572\n",
      "loss: 6.099339\n",
      "loss: 6.089144\n",
      "loss: 6.078988\n",
      "loss: 6.068869\n",
      "loss: 6.058788\n",
      "loss: 6.048746\n",
      "loss: 6.038741\n",
      "loss: 6.028772\n",
      "loss: 6.018840\n",
      "loss: 6.008946\n",
      "loss: 5.999088\n",
      "loss: 5.989266\n",
      "loss: 5.979481\n",
      "loss: 5.969731\n",
      "loss: 5.960018\n",
      "loss: 5.950341\n",
      "loss: 5.940697\n",
      "loss: 5.931090\n",
      "loss: 5.921518\n",
      "loss: 5.911980\n",
      "loss: 5.902476\n",
      "loss: 5.893008\n",
      "loss: 5.883573\n",
      "loss: 5.874173\n",
      "loss: 5.864806\n",
      "loss: 5.855473\n",
      "loss: 5.846173\n",
      "loss: 5.836906\n",
      "loss: 5.827673\n",
      "loss: 5.818472\n",
      "loss: 5.809305\n",
      "loss: 5.800169\n",
      "loss: 5.791066\n",
      "loss: 5.781995\n",
      "loss: 5.772957\n",
      "loss: 5.763950\n",
      "loss: 5.754976\n",
      "loss: 5.746032\n",
      "loss: 5.737120\n",
      "loss: 5.728240\n",
      "loss: 5.719389\n",
      "loss: 5.710571\n",
      "loss: 5.701782\n",
      "loss: 5.693025\n",
      "loss: 5.684297\n",
      "loss: 5.675600\n",
      "loss: 5.666932\n",
      "loss: 5.658295\n",
      "loss: 5.649688\n",
      "loss: 5.641109\n",
      "loss: 5.632560\n",
      "loss: 5.624041\n",
      "loss: 5.615551\n",
      "loss: 5.607091\n",
      "loss: 5.598657\n",
      "loss: 5.590253\n",
      "loss: 5.581878\n",
      "loss: 5.573531\n",
      "loss: 5.565212\n",
      "loss: 5.556920\n",
      "loss: 5.548658\n",
      "loss: 5.540423\n",
      "loss: 5.532215\n",
      "loss: 5.524035\n",
      "loss: 5.515882\n",
      "loss: 5.507757\n",
      "loss: 5.499659\n",
      "loss: 5.491587\n",
      "loss: 5.483542\n",
      "loss: 5.475524\n",
      "loss: 5.467533\n",
      "loss: 5.459568\n",
      "loss: 5.451628\n",
      "loss: 5.443715\n",
      "loss: 5.435829\n",
      "loss: 5.427967\n",
      "loss: 5.420132\n",
      "loss: 5.412323\n",
      "loss: 5.404538\n",
      "loss: 5.396779\n",
      "loss: 5.389045\n",
      "loss: 5.381337\n",
      "loss: 5.373653\n",
      "loss: 5.365993\n",
      "loss: 5.358359\n",
      "loss: 5.350750\n",
      "loss: 5.343165\n",
      "loss: 5.335604\n",
      "loss: 5.328067\n",
      "loss: 5.320555\n",
      "loss: 5.313066\n",
      "loss: 5.305601\n",
      "loss: 5.298160\n",
      "loss: 5.290743\n",
      "loss: 5.283349\n",
      "loss: 5.275978\n",
      "loss: 5.268631\n",
      "loss: 5.261307\n",
      "loss: 5.254006\n",
      "loss: 5.246728\n",
      "loss: 5.239473\n",
      "loss: 5.232241\n",
      "loss: 5.225030\n",
      "loss: 5.217843\n",
      "loss: 5.210678\n",
      "loss: 5.203535\n",
      "loss: 5.196415\n",
      "loss: 5.189316\n",
      "loss: 5.182240\n",
      "loss: 5.175186\n",
      "loss: 5.168152\n",
      "loss: 5.161141\n",
      "loss: 5.154152\n",
      "loss: 5.147184\n",
      "loss: 5.140237\n",
      "loss: 5.133311\n",
      "loss: 5.126406\n",
      "loss: 5.119523\n",
      "loss: 5.112661\n",
      "loss: 5.105819\n",
      "loss: 5.098998\n",
      "loss: 5.092197\n",
      "loss: 5.085417\n",
      "loss: 5.078658\n",
      "loss: 5.071918\n",
      "loss: 5.065200\n",
      "loss: 5.058502\n",
      "loss: 5.051824\n",
      "loss: 5.045165\n",
      "loss: 5.038527\n",
      "loss: 5.031908\n",
      "loss: 5.025309\n",
      "loss: 5.018730\n",
      "loss: 5.012169\n",
      "loss: 5.005630\n",
      "loss: 4.999107\n",
      "loss: 4.992606\n",
      "loss: 4.986123\n",
      "loss: 4.979660\n",
      "loss: 4.973215\n",
      "loss: 4.966789\n",
      "loss: 4.960382\n",
      "loss: 4.953994\n",
      "loss: 4.947624\n",
      "loss: 4.941273\n",
      "loss: 4.934940\n",
      "loss: 4.928627\n",
      "loss: 4.922330\n",
      "loss: 4.916053\n",
      "loss: 4.909793\n",
      "loss: 4.903552\n",
      "loss: 4.897328\n",
      "loss: 4.891123\n",
      "loss: 4.884935\n",
      "loss: 4.878765\n",
      "loss: 4.872612\n",
      "loss: 4.866478\n",
      "loss: 4.860360\n",
      "loss: 4.854261\n",
      "loss: 4.848179\n",
      "loss: 4.842113\n",
      "loss: 4.836065\n",
      "loss: 4.830035\n",
      "loss: 4.824021\n",
      "loss: 4.818024\n",
      "loss: 4.812044\n",
      "loss: 4.806081\n",
      "loss: 4.800134\n",
      "loss: 4.794204\n",
      "loss: 4.788291\n",
      "loss: 4.782395\n",
      "loss: 4.776514\n",
      "loss: 4.770650\n",
      "loss: 4.764803\n",
      "loss: 4.758971\n",
      "loss: 4.753157\n",
      "loss: 4.747357\n",
      "loss: 4.741574\n",
      "loss: 4.735807\n",
      "loss: 4.730056\n",
      "loss: 4.724320\n",
      "loss: 4.718601\n",
      "loss: 4.712897\n",
      "loss: 4.707209\n",
      "loss: 4.701536\n",
      "loss: 4.695879\n",
      "loss: 4.690237\n",
      "loss: 4.684610\n",
      "loss: 4.678999\n",
      "loss: 4.673403\n",
      "loss: 4.667821\n",
      "loss: 4.662256\n",
      "loss: 4.656704\n",
      "loss: 4.651169\n",
      "loss: 4.645648\n",
      "loss: 4.640141\n",
      "loss: 4.634650\n",
      "loss: 4.629173\n",
      "loss: 4.623711\n",
      "loss: 4.618264\n",
      "loss: 4.612831\n",
      "loss: 4.607412\n",
      "loss: 4.602008\n",
      "loss: 4.596618\n",
      "loss: 4.591243\n",
      "loss: 4.585882\n",
      "loss: 4.580534\n",
      "loss: 4.575202\n",
      "loss: 4.569882\n",
      "loss: 4.564578\n",
      "loss: 4.559287\n",
      "loss: 4.554009\n",
      "loss: 4.548745\n",
      "loss: 4.543496\n",
      "loss: 4.538260\n",
      "loss: 4.533037\n",
      "loss: 4.527829\n",
      "loss: 4.522634\n",
      "loss: 4.517452\n",
      "loss: 4.512284\n",
      "loss: 4.507129\n",
      "loss: 4.501987\n",
      "loss: 4.496859\n",
      "loss: 4.491743\n",
      "loss: 4.486641\n",
      "loss: 4.481553\n",
      "loss: 4.476477\n",
      "loss: 4.471414\n",
      "loss: 4.466363\n",
      "loss: 4.461326\n",
      "loss: 4.456302\n",
      "loss: 4.451290\n",
      "loss: 4.446292\n",
      "loss: 4.441306\n",
      "loss: 4.436332\n",
      "loss: 4.431371\n",
      "loss: 4.426423\n",
      "loss: 4.421487\n",
      "loss: 4.416564\n",
      "loss: 4.411653\n",
      "loss: 4.406753\n",
      "loss: 4.401866\n",
      "loss: 4.396991\n",
      "loss: 4.392129\n",
      "loss: 4.387279\n",
      "loss: 4.382441\n",
      "loss: 4.377614\n",
      "loss: 4.372801\n",
      "loss: 4.367999\n",
      "loss: 4.363208\n",
      "loss: 4.358430\n",
      "loss: 4.353664\n",
      "loss: 4.348908\n",
      "loss: 4.344165\n",
      "loss: 4.339434\n",
      "loss: 4.334713\n",
      "loss: 4.330006\n",
      "loss: 4.325309\n",
      "loss: 4.320624\n",
      "loss: 4.315949\n",
      "loss: 4.311287\n",
      "loss: 4.306635\n",
      "loss: 4.301996\n",
      "loss: 4.297368\n",
      "loss: 4.292750\n",
      "loss: 4.288144\n",
      "loss: 4.283549\n",
      "loss: 4.278965\n",
      "loss: 4.274391\n",
      "loss: 4.269829\n",
      "loss: 4.265278\n",
      "loss: 4.260737\n",
      "loss: 4.256208\n",
      "loss: 4.251690\n",
      "loss: 4.247182\n",
      "loss: 4.242685\n",
      "loss: 4.238198\n",
      "loss: 4.233723\n",
      "loss: 4.229258\n",
      "loss: 4.224803\n",
      "loss: 4.220359\n",
      "loss: 4.215926\n",
      "loss: 4.211502\n",
      "loss: 4.207090\n",
      "loss: 4.202687\n",
      "loss: 4.198296\n",
      "loss: 4.193914\n",
      "loss: 4.189543\n",
      "loss: 4.185182\n",
      "loss: 4.180831\n",
      "loss: 4.176490\n",
      "loss: 4.172160\n",
      "loss: 4.167839\n",
      "loss: 4.163528\n",
      "loss: 4.159227\n",
      "loss: 4.154937\n",
      "loss: 4.150657\n",
      "loss: 4.146385\n",
      "loss: 4.142125\n",
      "loss: 4.137873\n",
      "loss: 4.133632\n",
      "loss: 4.129400\n",
      "loss: 4.125179\n",
      "loss: 4.120966\n",
      "loss: 4.116763\n",
      "loss: 4.112570\n",
      "loss: 4.108387\n",
      "loss: 4.104212\n",
      "loss: 4.100048\n",
      "loss: 4.095892\n",
      "loss: 4.091747\n",
      "loss: 4.087611\n",
      "loss: 4.083484\n",
      "loss: 4.079366\n",
      "loss: 4.075258\n",
      "loss: 4.071159\n",
      "loss: 4.067069\n",
      "loss: 4.062988\n",
      "loss: 4.058917\n",
      "loss: 4.054854\n",
      "loss: 4.050801\n",
      "loss: 4.046757\n",
      "loss: 4.042721\n",
      "loss: 4.038695\n",
      "loss: 4.034677\n",
      "loss: 4.030668\n",
      "loss: 4.026669\n",
      "loss: 4.022678\n",
      "loss: 4.018697\n",
      "loss: 4.014724\n",
      "loss: 4.010759\n",
      "loss: 4.006804\n",
      "loss: 4.002857\n",
      "loss: 3.998919\n",
      "loss: 3.994989\n",
      "loss: 3.991068\n",
      "loss: 3.987155\n",
      "loss: 3.983252\n",
      "loss: 3.979356\n",
      "loss: 3.975469\n",
      "loss: 3.971590\n",
      "loss: 3.967720\n",
      "loss: 3.963858\n",
      "loss: 3.960004\n",
      "loss: 3.956159\n",
      "loss: 3.952322\n",
      "loss: 3.948493\n",
      "loss: 3.944673\n",
      "loss: 3.940861\n",
      "loss: 3.937057\n",
      "loss: 3.933261\n",
      "loss: 3.929473\n",
      "loss: 3.925694\n",
      "loss: 3.921922\n",
      "loss: 3.918159\n",
      "loss: 3.914404\n",
      "loss: 3.910656\n",
      "loss: 3.906917\n",
      "loss: 3.903185\n",
      "loss: 3.899461\n",
      "loss: 3.895746\n",
      "loss: 3.892038\n",
      "loss: 3.888337\n",
      "loss: 3.884645\n",
      "loss: 3.880960\n",
      "loss: 3.877283\n",
      "loss: 3.873614\n",
      "loss: 3.869953\n",
      "loss: 3.866299\n",
      "loss: 3.862653\n",
      "loss: 3.859014\n",
      "loss: 3.855384\n",
      "loss: 3.851760\n",
      "loss: 3.848144\n",
      "loss: 3.844536\n",
      "loss: 3.840935\n",
      "loss: 3.837341\n",
      "loss: 3.833755\n",
      "loss: 3.830176\n",
      "loss: 3.826604\n",
      "loss: 3.823041\n",
      "loss: 3.819484\n",
      "loss: 3.815935\n",
      "loss: 3.812393\n",
      "loss: 3.808859\n",
      "loss: 3.805331\n",
      "loss: 3.801811\n",
      "loss: 3.798298\n",
      "loss: 3.794791\n",
      "loss: 3.791293\n",
      "loss: 3.787800\n",
      "loss: 3.784316\n",
      "loss: 3.780838\n",
      "loss: 3.777367\n",
      "loss: 3.773904\n",
      "loss: 3.770447\n",
      "loss: 3.766997\n",
      "loss: 3.763554\n",
      "loss: 3.760119\n",
      "loss: 3.756690\n",
      "loss: 3.753268\n",
      "loss: 3.749853\n",
      "loss: 3.746444\n",
      "loss: 3.743042\n",
      "loss: 3.739648\n",
      "loss: 3.736259\n",
      "loss: 3.732878\n",
      "loss: 3.729504\n",
      "loss: 3.726136\n",
      "loss: 3.722774\n",
      "loss: 3.719420\n",
      "loss: 3.716072\n",
      "loss: 3.712731\n",
      "loss: 3.709396\n",
      "loss: 3.706068\n",
      "loss: 3.702746\n",
      "loss: 3.699430\n",
      "loss: 3.696122\n",
      "loss: 3.692820\n",
      "loss: 3.689524\n",
      "loss: 3.686235\n",
      "loss: 3.682952\n",
      "loss: 3.679675\n",
      "loss: 3.676405\n",
      "loss: 3.673141\n",
      "loss: 3.669884\n",
      "loss: 3.666632\n",
      "loss: 3.663388\n",
      "loss: 3.660149\n",
      "loss: 3.656917\n",
      "loss: 3.653690\n",
      "loss: 3.650470\n",
      "loss: 3.647256\n",
      "loss: 3.644049\n",
      "loss: 3.640848\n",
      "loss: 3.637652\n",
      "loss: 3.634463\n",
      "loss: 3.631280\n",
      "loss: 3.628103\n",
      "loss: 3.624932\n",
      "loss: 3.621767\n",
      "loss: 3.618608\n",
      "loss: 3.615455\n",
      "loss: 3.612308\n",
      "loss: 3.609167\n",
      "loss: 3.606032\n",
      "loss: 3.602902\n",
      "loss: 3.599779\n",
      "loss: 3.596661\n",
      "loss: 3.593550\n",
      "loss: 3.590444\n",
      "loss: 3.587344\n",
      "loss: 3.584251\n",
      "loss: 3.581162\n",
      "loss: 3.578080\n",
      "loss: 3.575003\n",
      "loss: 3.571931\n",
      "loss: 3.568866\n",
      "loss: 3.565806\n",
      "loss: 3.562752\n",
      "loss: 3.559704\n",
      "loss: 3.556661\n",
      "loss: 3.553624\n",
      "loss: 3.550593\n",
      "loss: 3.547567\n",
      "loss: 3.544547\n",
      "loss: 3.541532\n",
      "loss: 3.538522\n",
      "loss: 3.535519\n",
      "loss: 3.532521\n",
      "loss: 3.529528\n",
      "loss: 3.526541\n",
      "loss: 3.523559\n",
      "loss: 3.520583\n",
      "loss: 3.517612\n",
      "loss: 3.514647\n",
      "loss: 3.511686\n",
      "loss: 3.508732\n",
      "loss: 3.505782\n",
      "loss: 3.502838\n",
      "loss: 3.499899\n",
      "loss: 3.496966\n",
      "loss: 3.494037\n",
      "loss: 3.491114\n",
      "loss: 3.488197\n",
      "loss: 3.485284\n",
      "loss: 3.482377\n",
      "loss: 3.479475\n",
      "loss: 3.476578\n",
      "loss: 3.473686\n",
      "loss: 3.470799\n",
      "loss: 3.467918\n",
      "loss: 3.465042\n",
      "loss: 3.462171\n",
      "loss: 3.459305\n",
      "loss: 3.456444\n",
      "loss: 3.453588\n",
      "loss: 3.450737\n",
      "loss: 3.447891\n",
      "loss: 3.445050\n",
      "loss: 3.442215\n",
      "loss: 3.439384\n",
      "loss: 3.436559\n",
      "loss: 3.433738\n",
      "loss: 3.430922\n",
      "loss: 3.428112\n",
      "loss: 3.425305\n",
      "loss: 3.422504\n",
      "loss: 3.419708\n",
      "loss: 3.416916\n",
      "loss: 3.414130\n",
      "loss: 3.411349\n",
      "loss: 3.408571\n",
      "loss: 3.405800\n",
      "loss: 3.403033\n",
      "loss: 3.400270\n",
      "loss: 3.397512\n",
      "loss: 3.394759\n",
      "loss: 3.392012\n",
      "loss: 3.389268\n",
      "loss: 3.386529\n",
      "loss: 3.383796\n",
      "Classification Accuracy: 0.990000\n"
     ]
    }
   ],
   "source": [
    "n_steps = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Train model\n",
    "    for i in range(n_steps):\n",
    "        feed_dict = {x: x_np, y: y_np}\n",
    "        _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "        print(\"loss: %f\" % loss)\n",
    "        train_writer.add_summary(summary, i)\n",
    "        \n",
    "    # Get weights\n",
    "    w_final, b_final = sess.run([W, b])\n",
    "\n",
    "    # Make Predictions\n",
    "    y_pred_np = sess.run(y_pred, feed_dict={x: x_np})\n",
    "\n",
    "score = accuracy_score(y_np, y_pred_np)\n",
    "print(\"Classification Accuracy: %f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
