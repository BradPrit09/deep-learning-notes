{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/hakan/.pyenv/versions/miniconda3-latest/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADolJREFUeJzt3X2MXOV1x/HfyXq9jo1JvHVsHOJgxzgBYhqTjgzICFwhXKdCMqgCYkWRQ5M4LzgprStBraq4FancKiF1CUVamq1tifcEiv+gSZAVAVFhy+IQXuLwErMli7e7mA3YEOKX3dM/9m60MTvPrGfuzJ3d8/1I1szcc+/co4Hf3pl55t7H3F0A4nlP0Q0AKAbhB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1LRG7my6tfkMzWrkLoFQfqu3dcQP20TWrSn8ZrZG0jZJLZL+3d23ptafoVk61y6uZZcAErp894TXrfptv5m1SLpF0qcknSVpnZmdVe3zAWisWj7zr5D0krvvc/cjku6StDaftgDUWy3hP1XSr8Y87s2W/R4z22Bm3WbWfVSHa9gdgDzVEv7xvlR41/nB7t7h7iV3L7WqrYbdAchTLeHvlbRwzOMPSdpfWzsAGqWW8D8haamZLTaz6ZI+LWlXPm0BqLeqh/rc/ZiZbZT0Q40M9XW6+3O5dQagrmoa53f3ByU9mFMvABqIn/cCQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVE2z9JpZj6RDkoYkHXP3Uh5NIT82Lf2fuOUDc+u6/+f/elHZ2tDM4eS2py0ZSNZnftWS9f+7aXrZ2p7S3cltDwy9nayfe++mZP30v3o8WW8GNYU/88fufiCH5wHQQLztB4KqNfwu6Udm9qSZbcijIQCNUevb/pXuvt/M5kl6yMx+4e6PjF0h+6OwQZJmaGaNuwOQl5qO/O6+P7sdkHS/pBXjrNPh7iV3L7WqrZbdAchR1eE3s1lmNnv0vqTVkp7NqzEA9VXL2/75ku43s9HnucPdf5BLVwDqrurwu/s+SZ/IsZcpq+XMpcm6t7Um6/sven+y/s555cek29+XHq9+9BPp8e4i/ddvZifr//SdNcl619l3lK29fPSd5LZb+y9J1j/4qCfrkwFDfUBQhB8IivADQRF+ICjCDwRF+IGg8jirL7yhVZ9M1m/afkuy/tHW8qeeTmVHfShZ/7ubP5esT3s7Pdx2/r0by9Zmv3osuW3bgfRQ4MzurmR9MuDIDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc6fg7bn9yfrT/52YbL+0db+PNvJ1aa+85L1fW+lL/29fcn3ytbeHE6P08//1/9O1utp8p+wWxlHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IytwbN6J5srX7uXZxw/bXLAavPj9ZP7gmfXntlqdPStZ/9tWbT7inUTce+MNk/YmL0uP4Q2+8maz7+eWv7t7z9eSmWrzuZ+kV8C5dvlsHfTA9d3mGIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFVxnN/MOiVdKmnA3Zdly9ol3S1pkaQeSVe6+68r7SzqOH8lLXP/IFkfen0wWX/5jvJj9c9d2JncdsU/fi1Zn3dLcefU48TlPc6/XdLxE6FfL2m3uy+VtDt7DGASqRh+d39E0vGHnrWSdmT3d0i6LOe+ANRZtZ/557t7nyRlt/PyawlAI9T9Gn5mtkHSBkmaoZn13h2ACar2yN9vZgskKbsdKLeiu3e4e8ndS61qq3J3APJWbfh3SVqf3V8v6YF82gHQKBXDb2Z3SnpM0sfMrNfMPi9pq6RLzOxFSZdkjwFMIhU/87v7ujIlBuxzMnTg9Zq2P3pwetXbfvwzP0/WX7u1Jf0Ew0NV7xvF4hd+QFCEHwiK8ANBEX4gKMIPBEX4gaCYonsKOPO6F8rWrj47PSL7H6ftTtYvuuKaZH323Y8n62heHPmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjG+aeA1DTZr3/lzOS2r+x6J1m//sadyfrfXHl5su4/fV/Z2sJvPJbcVg2cPj4ijvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTFKbrzxBTdzWfwz89P1m+/4ZvJ+uJpM6re98d3bkzWl97Wl6wf29dT9b6nqryn6AYwBRF+ICjCDwRF+IGgCD8QFOEHgiL8QFAVx/nNrFPSpZIG3H1ZtmyLpC9Kei1bbbO7P1hpZ4zzTz6+cnmyfvLW3mT9zo/8sOp9n/HjLyTrH/v78tcxkKShF/dVve/JKu9x/u2S1oyz/Nvuvjz7VzH4AJpLxfC7+yOSBhvQC4AGquUz/0Yze9rMOs1sTm4dAWiIasN/q6QlkpZL6pP0rXIrmtkGM+s2s+6jOlzl7gDkrarwu3u/uw+5+7Ck2yStSKzb4e4ldy+1qq3aPgHkrKrwm9mCMQ8vl/RsPu0AaJSKl+42szslrZI018x6Jd0gaZWZLZfkknokfamOPQKoA87nR01a5s9L1vdfdXrZWtd125LbvqfCG9PPvLw6WX/zgteT9amI8/kBVET4gaAIPxAU4QeCIvxAUIQfCIqhPhTmnt70FN0zbXqy/hs/kqxf+rVryz/3/V3JbScrhvoAVET4gaAIPxAU4QeCIvxAUIQfCIrwA0FVPJ8fsQ1fkL509y+vSE/RvWx5T9lapXH8Sm4ePCdZn/lAd03PP9Vx5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnn+KstCxZf+Hr6bH221buSNYvnJE+p74Wh/1osv744OL0Ewz35djN1MORHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCqjjOb2YLJe2UdIqkYUkd7r7NzNol3S1pkaQeSVe6+6/r12pc0xaflqz/8uoPlq1tuequ5LZ/dtKBqnrKw+b+UrL+8LbzkvU5O9LX/UfaRI78xyRtcvczJZ0n6RozO0vS9ZJ2u/tSSbuzxwAmiYrhd/c+d9+T3T8kaa+kUyWtlTT6868dki6rV5MA8ndCn/nNbJGkcyR1SZrv7n3SyB8ISfPybg5A/Uw4/GZ2kqTvS7rW3Q+ewHYbzKzbzLqP6nA1PQKogwmF38xaNRL82939vmxxv5ktyOoLJA2Mt627d7h7yd1LrWrLo2cAOagYfjMzSd+VtNfdbxpT2iVpfXZ/vaQH8m8PQL1M5JTelZI+K+kZM3sqW7ZZ0lZJ95jZ5yW9IumK+rQ4+U1b9OFk/c0/WpCsX/UPP0jWv/z++5L1etrUlx6Oe+zfyg/ntW//n+S2c4YZyquniuF3959IKjff98X5tgOgUfiFHxAU4QeCIvxAUIQfCIrwA0ERfiAoLt09QdMWnFK2Ntg5K7ntVxY/nKyvm91fVU952PjqBcn6nlvTU3TP/d6zyXr7IcbqmxVHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IKsw4/5E/SV8m+shfDibrm09/sGxt9XvfrqqnvPQPvVO2duGuTcltz/jbXyTr7W+kx+mHk1U0M478QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUmHH+nsvSf+deOPveuu37ljeWJOvbHl6drNtQuSunjzjjxpfL1pb2dyW3HUpWMZVx5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoMzd0yuYLZS0U9IpGjl9u8Pdt5nZFklflPRatupmdy9/0rukk63dzzVm9Qbqpct366APpn8YkpnIj3yOSdrk7nvMbLakJ83soaz2bXf/ZrWNAihOxfC7e5+kvuz+ITPbK+nUejcGoL5O6DO/mS2SdI6k0d+MbjSzp82s08zmlNlmg5l1m1n3UR2uqVkA+Zlw+M3sJEnfl3Stux+UdKukJZKWa+SdwbfG287dO9y95O6lVrXl0DKAPEwo/GbWqpHg3+7u90mSu/e7+5C7D0u6TdKK+rUJIG8Vw29mJum7kva6+01jli8Ys9rlktLTtQJoKhP5tn+lpM9KesbMnsqWbZa0zsyWS3JJPZK+VJcOAdTFRL7t/4mk8cYNk2P6AJobv/ADgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EVfHS3bnuzOw1Sf87ZtFcSQca1sCJadbemrUvid6qlWdvp7n7ByayYkPD/66dm3W7e6mwBhKatbdm7Uuit2oV1Rtv+4GgCD8QVNHh7yh4/ynN2luz9iXRW7UK6a3Qz/wAilP0kR9AQQoJv5mtMbPnzewlM7u+iB7KMbMeM3vGzJ4ys+6Ce+k0swEze3bMsnYze8jMXsxux50mraDetpjZq9lr95SZ/WlBvS00sx+b2V4ze87M/iJbXuhrl+irkNet4W/7zaxF0guSLpHUK+kJSevc/ecNbaQMM+uRVHL3wseEzexCSW9J2unuy7Jl/yxp0N23Zn8457j7dU3S2xZJbxU9c3M2ocyCsTNLS7pM0udU4GuX6OtKFfC6FXHkXyHpJXff5+5HJN0laW0BfTQ9d39E0uBxi9dK2pHd36GR/3karkxvTcHd+9x9T3b/kKTRmaULfe0SfRWiiPCfKulXYx73qrmm/HZJPzKzJ81sQ9HNjGN+Nm366PTp8wru53gVZ25upONmlm6a166aGa/zVkT4x5v9p5mGHFa6+yclfUrSNdnbW0zMhGZubpRxZpZuCtXOeJ23IsLfK2nhmMcfkrS/gD7G5e77s9sBSfer+WYf7h+dJDW7HSi4n99pppmbx5tZWk3w2jXTjNdFhP8JSUvNbLGZTZf0aUm7CujjXcxsVvZFjMxslqTVar7Zh3dJWp/dXy/pgQJ7+T3NMnNzuZmlVfBr12wzXhfyI59sKONfJLVI6nT3bzS8iXGY2Uc0crSXRiYxvaPI3szsTkmrNHLWV7+kGyT9p6R7JH1Y0iuSrnD3hn/xVqa3VRp56/q7mZtHP2M3uLcLJD0q6RlJw9nizRr5fF3Ya5foa50KeN34hR8QFL/wA4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1P8D6+E2hIAP97kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_data[0,:,:])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.astype('float32')\n",
    "test_data = test_data.astype('float32')\n",
    "train_data /= 255\n",
    "test_data /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "\n",
    "# Generate a validation set.\n",
    "validation_data = train_data[:VALIDATION_SIZE, ...]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, ...]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.nn.conv2d(\n",
    "    input,\n",
    "    filter,\n",
    "    strides,\n",
    "    padding,\n",
    "    use_cudnn_on_gpu=None,\n",
    "    data_format=None,\n",
    "    name=None\n",
    ")\n",
    "```\n",
    "\n",
    "`input` is assumed to be a tensor of shape `(batch, height, width, channels)` where `batch` is the number of images in a minibatch.\n",
    "\n",
    "The argument `filter` is a tensor of shape `(filter_height, filter_width, channels, out_channels)`\n",
    "\n",
    "`strides` contains the filter strides and is a list of length 4 (one for each input dimension).\n",
    "\n",
    "```\n",
    "tf.nn.max_pool(\n",
    "    value,\n",
    "    ksize,\n",
    "    strides,\n",
    "    padding,\n",
    "    data_format='NHWC',\n",
    "    name=None\n",
    ")\n",
    "```\n",
    "\n",
    "`value` has the same shape as input for `tf.nn.conv2d`, `(batch, height, width, channels)`. `ksize` is the size of the pooling window and is a list of length 4. `strides` and `padding` behave as for `tf.nn.conv2d`.\n",
    "\n",
    "Letâ€™s define the weights needed to train our LeNet-5 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 1\n",
    "IMAGE_SIZE = 28\n",
    "NUM_LABELS = 10\n",
    "SEED = 66478  # Set to None for random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "        # This is where training samples and labels are fed to the graph.\n",
    "        # These placeholder nodes will be fed a batch of training data at each\n",
    "        # training step using the {feed_dict} argument to the Run() call below.\n",
    "\n",
    "        conv1_weights = tf.Variable(\n",
    "            tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                                stddev=0.1,\n",
    "                                seed=SEED, dtype=tf.float32), name='conv1_weights')\n",
    "        \n",
    "        conv1_biases = tf.Variable(tf.zeros([32], dtype=tf.float32), name='conv1_biases')\n",
    "\n",
    "        conv2_weights = tf.Variable(tf.truncated_normal(\n",
    "                                [5, 5, 32, 64], stddev=0.1, # 5x5 filter, depth 64, number channel 32\n",
    "                                seed=SEED, dtype=tf.float32), name='conv2_weights')\n",
    "        \n",
    "        conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=tf.float32), name='conv2_biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "        tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                            stddev=0.1,\n",
    "                            seed=SEED,\n",
    "                            dtype=tf.float32), name='fc1_weights')\n",
    "    fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=tf.float32), name='fc1_biases')\n",
    "    fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],\n",
    "                                                  stddev=0.1,\n",
    "                                                  seed=SEED,\n",
    "                                                  dtype=tf.float32), name='fc2_weights')\n",
    "    fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype=tf.float32), name='fc2_biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    with graph.as_default():\n",
    "        # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "        # the same size as the input). Note that {strides} is a 4D array whose\n",
    "        # shape matches the data layout: [image index, y, x, depth].\n",
    "        conv = tf.nn.conv2d(data,\n",
    "                          conv1_weights,\n",
    "                          strides=[1, 1, 1, 1],\n",
    "                          padding='SAME')\n",
    "        # Bias and rectified linear non-linearity.\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "        # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "        # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "        pool = tf.nn.max_pool(relu,\n",
    "                            ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "        conv = tf.nn.conv2d(pool,\n",
    "                          conv2_weights,\n",
    "                          strides=[1, 1, 1, 1],\n",
    "                          padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "        pool = tf.nn.max_pool(relu,\n",
    "                            ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "        # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "        # fully connected layers.\n",
    "        pool_shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(\n",
    "            pool,\n",
    "            [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "\n",
    "        # Fully connected layer. Note that the '+' operation automatically\n",
    "        # broadcasts the biases.\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "        # Add a 50% dropout during training only. Dropout also scales\n",
    "        # activations such that no rescaling is needed at evaluation time.\n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "\n",
    "        # output dot product + bias =  fc layer\n",
    "        return tf.matmul(hidden, fc2_weights) + fc2_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EVAL_BATCH_SIZE = 64\n",
    "\n",
    "with graph.as_default():\n",
    "    train_data_node = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS), name='train_data_node')\n",
    "    \n",
    "    train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,), name='train_labels_node')\n",
    "    \n",
    "    eval_data = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS), name='eval_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100  # Number of steps between evaluations.\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "train_size = train_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Training computation: logits + cross-entropy loss.\n",
    "    logits = model(train_data_node, True)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=train_labels_node, logits=logits))\n",
    "\n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = (tf.nn.l2_loss(fc1_weights)\n",
    "                    + tf.nn.l2_loss(fc1_biases)\n",
    "                    + tf.nn.l2_loss(fc2_weights)\n",
    "                    + tf.nn.l2_loss(fc2_biases))\n",
    "    # Add the regularization term to the loss.\n",
    "    loss += 5e-4 * regularizers\n",
    "\n",
    "    # Optimizer: set up a variable that's incremented once per batch and\n",
    "    # controls the learning rate decay.\n",
    "    batch = tf.Variable(0, dtype=tf.float32)\n",
    "    \n",
    "    # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.01,                # Base learning rate.\n",
    "        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "        train_size,          # Decay step.\n",
    "        0.95,                # Decay rate.\n",
    "        staircase=True)\n",
    "    \n",
    "    # Use simple momentum for the optimization.\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "    \n",
    "    # Predictions for the current training minibatch.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Predictions for the test and validation, which we'll compute less\n",
    "    # often.\n",
    "    eval_prediction = tf.nn.softmax(model(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n",
    "    return 100.0 - (\n",
    "      100.0 *\n",
    "      np.sum(np.argmax(predictions, 1) == labels) /\n",
    "      predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small utility function to evaluate a dataset by feeding batches of\n",
    "# data to {eval_data} and pulling the results from {eval_predictions}.\n",
    "# Saves memory and enables this to run on smaller GPUs.\n",
    "def eval_in_batches(data, sess):\n",
    "    \"\"\"Get predictions for a dataset by running it in small batches.\"\"\"\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "        raise ValueError(\"batch size for evals larger than dataset: %d\"\n",
    "                     % size)\n",
    "    predictions = np.ndarray(shape=(size, NUM_LABELS), dtype=np.float32)\n",
    "    \n",
    "    for begin in range(0, size, EVAL_BATCH_SIZE):\n",
    "        end = begin + EVAL_BATCH_SIZE\n",
    "        if end <= size:\n",
    "            predictions[begin:end, :] = sess.run(\n",
    "              eval_prediction,\n",
    "              feed_dict={eval_data: data[begin:end, ...]})\n",
    "            \n",
    "        else:\n",
    "            batch_predictions = sess.run(\n",
    "              eval_prediction,\n",
    "              feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "            \n",
    "            predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (epoch 0.00), 4.4 ms\n",
      "Minibatch loss: 8.280, learning rate: 0.010000\n",
      "Minibatch error: 84.4%\n",
      "Validation error: 76.8%\n",
      "Test error: 77.7%\n",
      "Test error: 76.0%\n",
      "Test error: 83.0%\n",
      "Test error: 80.5%\n",
      "Test error: 73.7%\n",
      "Test error: 75.0%\n",
      "Test error: 74.2%\n",
      "Test error: 71.5%\n",
      "Test error: 71.4%\n",
      "Test error: 73.8%\n",
      "Test error: 74.8%\n",
      "Test error: 74.5%\n",
      "Test error: 71.7%\n",
      "Test error: 68.0%\n",
      "Test error: 63.1%\n",
      "Test error: 58.6%\n",
      "Test error: 57.1%\n",
      "Test error: 54.3%\n",
      "Test error: 47.2%\n",
      "Test error: 41.2%\n",
      "Test error: 39.5%\n",
      "Test error: 39.1%\n",
      "Test error: 37.2%\n",
      "Test error: 35.0%\n",
      "Test error: 33.5%\n",
      "Test error: 32.2%\n",
      "Test error: 31.5%\n",
      "Test error: 29.2%\n",
      "Test error: 26.7%\n",
      "Test error: 23.3%\n",
      "Test error: 21.4%\n",
      "Test error: 20.2%\n",
      "Test error: 19.3%\n",
      "Test error: 18.1%\n",
      "Test error: 17.0%\n",
      "Test error: 17.4%\n",
      "Test error: 17.7%\n",
      "Test error: 17.5%\n",
      "Test error: 15.7%\n",
      "Test error: 16.3%\n",
      "Test error: 17.7%\n",
      "Test error: 17.2%\n",
      "Test error: 15.8%\n",
      "Test error: 14.9%\n",
      "Test error: 14.0%\n",
      "Test error: 13.1%\n",
      "Test error: 13.4%\n",
      "Test error: 14.5%\n",
      "Test error: 14.1%\n",
      "Test error: 12.1%\n",
      "Test error: 11.5%\n",
      "Test error: 11.4%\n",
      "Test error: 12.1%\n",
      "Test error: 12.0%\n",
      "Test error: 11.2%\n",
      "Test error: 10.3%\n",
      "Test error: 9.8%\n",
      "Test error: 10.0%\n",
      "Test error: 9.8%\n",
      "Test error: 9.9%\n",
      "Test error: 9.6%\n",
      "Test error: 9.6%\n",
      "Test error: 9.8%\n",
      "Test error: 10.1%\n",
      "Test error: 10.5%\n",
      "Test error: 10.8%\n",
      "Test error: 10.6%\n",
      "Test error: 10.0%\n",
      "Test error: 9.1%\n",
      "Test error: 8.5%\n",
      "Test error: 8.3%\n",
      "Test error: 8.0%\n",
      "Test error: 7.8%\n",
      "Test error: 7.5%\n",
      "Test error: 7.2%\n",
      "Test error: 7.8%\n",
      "Test error: 9.2%\n",
      "Test error: 10.2%\n",
      "Test error: 10.8%\n",
      "Test error: 10.4%\n",
      "Test error: 9.5%\n",
      "Test error: 8.0%\n",
      "Test error: 7.6%\n",
      "Test error: 8.1%\n",
      "Test error: 9.5%\n",
      "Test error: 10.4%\n",
      "Test error: 11.0%\n",
      "Test error: 10.7%\n",
      "Test error: 10.3%\n",
      "Test error: 9.4%\n",
      "Test error: 8.3%\n",
      "Test error: 7.6%\n",
      "Test error: 7.3%\n",
      "Test error: 7.2%\n",
      "Test error: 7.5%\n",
      "Test error: 7.7%\n",
      "Test error: 7.5%\n",
      "Test error: 7.6%\n",
      "Test error: 7.3%\n",
      "Test error: 7.2%\n",
      "Step 100 (epoch 0.12), 6901.8 ms\n",
      "Minibatch loss: 3.218, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 6.7%\n",
      "Test error: 7.2%\n",
      "Test error: 7.1%\n",
      "Test error: 6.9%\n",
      "Test error: 7.0%\n",
      "Test error: 7.2%\n",
      "Test error: 7.5%\n",
      "Test error: 7.8%\n",
      "Test error: 7.8%\n",
      "Test error: 7.5%\n",
      "Test error: 7.2%\n",
      "Test error: 6.8%\n",
      "Test error: 6.5%\n",
      "Test error: 6.5%\n",
      "Test error: 6.3%\n",
      "Test error: 6.3%\n",
      "Test error: 6.1%\n",
      "Test error: 6.2%\n",
      "Test error: 6.4%\n",
      "Test error: 6.7%\n",
      "Test error: 7.2%\n",
      "Test error: 7.4%\n",
      "Test error: 7.5%\n",
      "Test error: 7.7%\n",
      "Test error: 7.5%\n",
      "Test error: 6.8%\n",
      "Test error: 6.2%\n",
      "Test error: 5.6%\n",
      "Test error: 5.4%\n",
      "Test error: 5.3%\n",
      "Test error: 5.4%\n",
      "Test error: 5.5%\n",
      "Test error: 5.7%\n",
      "Test error: 5.8%\n",
      "Test error: 5.7%\n",
      "Test error: 5.6%\n",
      "Test error: 5.5%\n",
      "Test error: 5.2%\n",
      "Test error: 5.0%\n",
      "Test error: 5.0%\n",
      "Test error: 4.9%\n",
      "Test error: 5.0%\n",
      "Test error: 4.9%\n",
      "Test error: 4.9%\n",
      "Test error: 4.9%\n",
      "Test error: 5.0%\n",
      "Test error: 4.9%\n",
      "Test error: 4.8%\n",
      "Test error: 4.7%\n",
      "Test error: 4.7%\n",
      "Test error: 4.6%\n",
      "Test error: 4.5%\n",
      "Test error: 4.4%\n",
      "Test error: 4.5%\n",
      "Test error: 4.5%\n",
      "Test error: 4.6%\n",
      "Test error: 4.6%\n",
      "Test error: 4.7%\n",
      "Test error: 4.5%\n",
      "Test error: 4.4%\n",
      "Test error: 4.3%\n",
      "Test error: 4.2%\n",
      "Test error: 4.1%\n",
      "Test error: 4.0%\n",
      "Test error: 4.0%\n",
      "Test error: 4.0%\n",
      "Test error: 4.0%\n",
      "Test error: 4.0%\n",
      "Test error: 4.0%\n",
      "Test error: 3.9%\n",
      "Test error: 4.0%\n",
      "Test error: 4.0%\n",
      "Test error: 4.0%\n",
      "Test error: 3.9%\n",
      "Test error: 4.0%\n",
      "Test error: 4.2%\n",
      "Test error: 4.6%\n",
      "Test error: 4.6%\n",
      "Test error: 4.5%\n",
      "Test error: 4.4%\n",
      "Test error: 4.3%\n",
      "Test error: 4.2%\n",
      "Test error: 4.1%\n",
      "Test error: 4.1%\n",
      "Test error: 3.9%\n",
      "Test error: 3.7%\n",
      "Test error: 3.7%\n",
      "Test error: 3.7%\n",
      "Test error: 3.7%\n",
      "Test error: 3.7%\n",
      "Test error: 3.8%\n",
      "Test error: 3.8%\n",
      "Test error: 3.9%\n",
      "Test error: 3.9%\n",
      "Test error: 3.8%\n",
      "Test error: 3.7%\n",
      "Test error: 3.6%\n",
      "Test error: 3.6%\n",
      "Test error: 3.7%\n",
      "Test error: 3.8%\n",
      "Test error: 3.9%\n",
      "Step 200 (epoch 0.23), 6590.2 ms\n",
      "Minibatch loss: 3.354, learning rate: 0.010000\n",
      "Minibatch error: 10.9%\n",
      "Validation error: 3.9%\n",
      "Test error: 3.9%\n",
      "Test error: 4.0%\n",
      "Test error: 3.9%\n",
      "Test error: 3.7%\n",
      "Test error: 3.6%\n",
      "Test error: 3.5%\n",
      "Test error: 3.4%\n",
      "Test error: 3.3%\n",
      "Test error: 3.3%\n",
      "Test error: 3.3%\n",
      "Test error: 3.5%\n",
      "Test error: 3.7%\n",
      "Test error: 3.7%\n",
      "Test error: 3.8%\n",
      "Test error: 3.8%\n",
      "Test error: 3.7%\n",
      "Test error: 3.6%\n",
      "Test error: 3.5%\n",
      "Test error: 3.3%\n",
      "Test error: 3.3%\n",
      "Test error: 3.4%\n",
      "Test error: 3.4%\n",
      "Test error: 3.5%\n",
      "Test error: 3.5%\n",
      "Test error: 3.4%\n",
      "Test error: 3.4%\n",
      "Test error: 3.4%\n",
      "Test error: 3.3%\n",
      "Test error: 3.2%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.3%\n",
      "Test error: 3.4%\n",
      "Test error: 3.5%\n",
      "Test error: 3.6%\n",
      "Test error: 3.5%\n",
      "Test error: 3.5%\n",
      "Test error: 3.5%\n",
      "Test error: 3.4%\n",
      "Test error: 3.4%\n",
      "Test error: 3.4%\n",
      "Test error: 3.5%\n",
      "Test error: 3.5%\n",
      "Test error: 3.5%\n",
      "Test error: 3.5%\n",
      "Test error: 3.4%\n",
      "Test error: 3.5%\n",
      "Test error: 3.5%\n",
      "Test error: 3.5%\n",
      "Test error: 3.5%\n",
      "Test error: 3.4%\n",
      "Test error: 3.2%\n",
      "Test error: 3.1%\n",
      "Test error: 3.2%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.2%\n",
      "Test error: 3.3%\n",
      "Test error: 3.4%\n",
      "Test error: 3.5%\n",
      "Test error: 3.5%\n",
      "Test error: 3.3%\n",
      "Test error: 3.2%\n",
      "Test error: 3.2%\n",
      "Test error: 3.5%\n",
      "Test error: 3.9%\n",
      "Test error: 4.5%\n",
      "Test error: 4.8%\n",
      "Test error: 4.7%\n",
      "Test error: 4.2%\n",
      "Test error: 4.5%\n",
      "Test error: 5.0%\n",
      "Test error: 5.3%\n",
      "Test error: 5.6%\n",
      "Test error: 5.6%\n",
      "Test error: 5.3%\n",
      "Test error: 4.8%\n",
      "Test error: 4.2%\n",
      "Test error: 4.0%\n",
      "Test error: 3.7%\n",
      "Test error: 3.7%\n",
      "Test error: 3.6%\n",
      "Test error: 3.6%\n",
      "Test error: 3.6%\n",
      "Test error: 3.5%\n",
      "Test error: 3.4%\n",
      "Test error: 3.4%\n",
      "Test error: 3.5%\n",
      "Test error: 3.4%\n",
      "Test error: 3.5%\n",
      "Test error: 3.5%\n",
      "Test error: 3.4%\n",
      "Test error: 3.3%\n",
      "Test error: 3.4%\n",
      "Test error: 3.3%\n",
      "Step 300 (epoch 0.35), 6578.0 ms\n",
      "Minibatch loss: 3.178, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 3.3%\n",
      "Test error: 3.1%\n",
      "Test error: 3.0%\n",
      "Test error: 3.0%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 3.0%\n",
      "Test error: 3.0%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.2%\n",
      "Test error: 3.2%\n",
      "Test error: 3.2%\n",
      "Test error: 3.2%\n",
      "Test error: 3.2%\n",
      "Test error: 3.0%\n",
      "Test error: 3.1%\n",
      "Test error: 3.0%\n",
      "Test error: 3.0%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.2%\n",
      "Test error: 3.2%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.2%\n",
      "Test error: 3.1%\n",
      "Test error: 3.1%\n",
      "Test error: 3.0%\n",
      "Test error: 3.1%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.6%\n",
      "Test error: 2.7%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.9%\n",
      "Test error: 3.0%\n",
      "Test error: 3.1%\n",
      "Test error: 3.2%\n",
      "Test error: 3.3%\n",
      "Test error: 3.3%\n",
      "Test error: 3.2%\n",
      "Test error: 3.1%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 3.0%\n",
      "Test error: 3.0%\n",
      "Test error: 3.0%\n",
      "Test error: 3.0%\n",
      "Test error: 2.9%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.7%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.9%\n",
      "Test error: 3.0%\n",
      "Test error: 3.2%\n",
      "Test error: 3.2%\n",
      "Test error: 3.1%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.7%\n",
      "Test error: 2.6%\n",
      "Test error: 2.6%\n",
      "Test error: 2.6%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.8%\n",
      "Test error: 2.9%\n",
      "Test error: 2.8%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Step 400 (epoch 0.47), 6570.6 ms\n",
      "Minibatch loss: 3.317, learning rate: 0.010000\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.6%\n",
      "Test error: 2.6%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.6%\n",
      "Test error: 2.7%\n",
      "Test error: 2.8%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 3.0%\n",
      "Test error: 2.9%\n",
      "Test error: 2.8%\n",
      "Test error: 2.7%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.6%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.6%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 3.0%\n",
      "Test error: 3.0%\n",
      "Test error: 3.0%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.8%\n",
      "Test error: 2.7%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.3%\n",
      "Test error: 2.3%\n",
      "Test error: 2.3%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.5%\n",
      "Step 500 (epoch 0.58), 6596.9 ms\n",
      "Minibatch loss: 3.267, learning rate: 0.010000\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 2.9%\n",
      "Test error: 2.6%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.6%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.8%\n",
      "Test error: 2.7%\n",
      "Test error: 2.8%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.4%\n",
      "Test error: 2.5%\n",
      "Test error: 2.6%\n",
      "Test error: 2.6%\n",
      "Test error: 2.7%\n",
      "Test error: 2.8%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 3.0%\n",
      "Test error: 3.0%\n",
      "Test error: 2.9%\n",
      "Test error: 2.7%\n",
      "Test error: 2.6%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.6%\n",
      "Test error: 2.6%\n",
      "Test error: 2.7%\n",
      "Test error: 2.8%\n",
      "Test error: 2.9%\n",
      "Test error: 2.8%\n",
      "Test error: 2.7%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.5%\n",
      "Test error: 2.7%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 2.9%\n",
      "Test error: 3.0%\n",
      "Test error: 3.0%\n",
      "Test error: 3.0%\n",
      "Test error: 2.9%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.5%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.5%\n",
      "Test error: 2.4%\n",
      "Test error: 2.5%\n",
      "Test error: 2.6%\n",
      "Test error: 2.6%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.6%\n",
      "Test error: 2.5%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.3%\n",
      "Test error: 2.3%\n",
      "Test error: 2.3%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.4%\n",
      "Test error: 2.3%\n",
      "Test error: 2.5%\n",
      "Test error: 2.6%\n",
      "Test error: 2.6%\n",
      "Test error: 2.7%\n",
      "Test error: 2.7%\n"
     ]
    }
   ],
   "source": [
    "# Create a local session to run the training.\n",
    "start_time = time.time()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "  # Run all the initializers to prepare the trainable parameters.\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "  # Loop through training steps.\n",
    "  for step in range(600):\n",
    "    # Compute the offset of the current minibatch in the data.\n",
    "    # Note that we could use better randomization across epochs.\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    \n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    \n",
    "    # This dictionary maps the batch data (as a NumPy array) to the\n",
    "    # node in the graph it should be fed to.\n",
    "    feed_dict = {train_data_node: batch_data,\n",
    "                 train_labels_node: batch_labels}\n",
    "    # Run the optimizer to update weights.\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "    \n",
    "    # print some extra information once reach the evaluation frequency\n",
    "    if step % EVAL_FREQUENCY == 0:\n",
    "        # fetch some extra nodes' data\n",
    "        l, lr, predictions = sess.run([loss, learning_rate,\n",
    "                                     train_prediction],\n",
    "                                    feed_dict=feed_dict)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        print('Step %d (epoch %.2f), %.1f ms' %\n",
    "            (step, float(step) * BATCH_SIZE / train_size,\n",
    "             1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "        print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "        print('Minibatch error: %.1f%%'\n",
    "            % error_rate(predictions, batch_labels))\n",
    "        print('Validation error: %.1f%%' % error_rate(\n",
    "          eval_in_batches(validation_data, sess), validation_labels))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    # Finally print the result!\n",
    "    test_error = error_rate(eval_in_batches(test_data, sess),\n",
    "                          test_labels)\n",
    "    print('Test error: %.1f%%' % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
