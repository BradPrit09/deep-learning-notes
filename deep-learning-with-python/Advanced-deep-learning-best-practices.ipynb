{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOING BEYOND THE SEQUENTIAL MODEL: THE KERAS FUNCTIONAL API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the functional API, you directly manipulate tensors, and you use layers as functions that take tensors and return tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hakan/.pyenv/versions/3.6.4/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, layers\n",
    "\n",
    "input_tensor = Input(shape=(32,))                   \n",
    "dense = layers.Dense(32, activation='relu')         \n",
    "\n",
    "output_tensor = dense(input_tensor)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "\n",
    "seq_model = Sequential()                                               \n",
    "seq_model.add(Dense(32, activation='relu', input_shape=(64,)))\n",
    "seq_model.add(Dense(32, activation='relu'))\n",
    "seq_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "input_tensor = Input(shape=(64,))                                      \n",
    "x = Dense(32, activation='relu')(input_tensor)                  \n",
    "x = Dense(32, activation='relu')(x)                             \n",
    "output_tensor = Dense(10, activation='softmax')(x)              \n",
    "\n",
    "model = Model(input_tensor, output_tensor)                             \n",
    "\n",
    "model.summary()                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrelated_input = Input(shape=(32,))\n",
    "bad_model = model = Model(unrelated_input, output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 304us/step - loss: 11.6280\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 11.5843\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 11.5756\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.5705\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.5665\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.5640\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 11.5620\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 11.5602\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 11.5584\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.5573\n",
      "1000/1000 [==============================] - 0s 89us/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "import numpy as np                                                 \n",
    "x_train = np.random.random((1000, 64))\n",
    "y_train = np.random.random((1000, 10))\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)                   \n",
    "\n",
    "score = model.evaluate(x_train, y_train)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-input models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 10000)  640000      text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 10000)  320000      question[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 32)           1284224     embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 16)           641088      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 48)           0           lstm_3[0][0]                     \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 500)          24500       concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,909,812\n",
      "Trainable params: 2,909,812\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500\n",
    "\n",
    "\n",
    "text_input = Input(shape=(None,), dtype='int32', name='text')            \n",
    "embedded_text = layers.Embedding(64, text_vocabulary_size)(text_input)                                \n",
    "encoded_text = layers.LSTM(32)(embedded_text)                            \n",
    "\n",
    "question_input = Input(shape=(None,),dtype='int32', name='question')                                  \n",
    "embedded_question = layers.Embedding(32, question_vocabulary_size)(question_input)\n",
    "encoded_question = layers.LSTM(16)(embedded_question)\n",
    "\n",
    "\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1)                               \n",
    "\n",
    "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)                \n",
    "\n",
    "model = Model([text_input, question_input], answer)                      \n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))                   \n",
    "question = np.random.randint(1, question_vocabulary_size, size=(num_samples, max_length))\n",
    "answers = np.random.randint(0, 1,size=(num_samples, answer_vocabulary_size))    \n",
    "\n",
    "model.fit([text, question], answers, epochs=10, batch_size=128)            \n",
    "# or\n",
    "model.fit({'text': text, 'question': question}, answers,epochs=10, batch_size=128)                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-output models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hakan/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Embedding\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "\n",
    "vocabulary_size = 50000\n",
    "num_income_groups = 10\n",
    "\n",
    "posts_input = Input(shape=(None,), dtype='int32', name='posts')\n",
    "embedded_posts = Embedding(256, vocabulary_size)(posts_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_posts)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(256, 5, activation='relu')(x)\n",
    "x = Conv1D(256, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(256, 5, activation='relu')(x)\n",
    "x = Conv1D(256, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "\n",
    "age_prediction = Dense(1, name='age')(x)               \n",
    "income_prediction = Dense(num_income_groups,\n",
    "                                 activation='softmax',\n",
    "                                 name='income')(x)\n",
    "gender_prediction = Dense(1, activation='sigmoid', name='gender')(x)\n",
    "\n",
    "model = Model(posts_input,\n",
    "              [age_prediction, income_prediction, gender_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=[ 'mse', 'categorical_crossentropy', 'binary_crossentropy'])\n",
    "\n",
    "# or \n",
    "\n",
    "model.compile(optimizer='rmsprop',                             \n",
    "              loss={'age': 'mse',                              \n",
    "                    'income': 'categorical_crossentropy',      \n",
    "                    'gender': 'binary_crossentropy'})          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that very imbalanced loss contributions will cause the model representations to be optimized preferentially for the task with the largest individual loss, at the expense of the other tasks.  You can assign different levels of importance to the loss values in their contribution to the final loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'],\n",
    "              loss_weights=[0.25, 1., 10.])\n",
    "\n",
    "# or\n",
    "\n",
    "model.compile(optimizer='rmsprop',                            \n",
    "              loss={'age': 'mse',                             \n",
    "                    'income': 'categorical_crossentropy',     \n",
    "                    'gender': 'binary_crossentropy'},         \n",
    "              loss_weights={'age': 0.25,                      \n",
    "                            'income': 1.,                     \n",
    "                            'gender': 10.})                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(posts, [age_targets, income_targets, gender_targets],        \n",
    "          epochs=10, batch_size=64)\n",
    "\n",
    "model.fit(posts, {'age': age_targets,                                  \n",
    "                  'income': income_targets,                            \n",
    "                  'gender': gender_targets},                           \n",
    "          epochs=10, batch_size=64)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer weight sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense \n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "import keras.layers.concatenate as con\n",
    "\n",
    "lstm = LSTM(32)                                                \n",
    "left_input = Input(shape=(None, 128))                                 \n",
    "left_output = lstm(left_input)                                        \n",
    "\n",
    "right_input = Input(shape=(None, 128))                                \n",
    "right_output = lstm(right_input)                                      \n",
    "\n",
    "merged = con([left_output, right_output], axis=-1)     \n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)           \n",
    "\n",
    "model = Model([left_input, right_input], predictions)  \n",
    "\n",
    "model.fit([left_data, right_data], targets...)                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, y2 = model([x1, x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import applications\n",
    "from keras import Input\n",
    "\n",
    "xception_base = applications.Xception(weights=None,include_top=False)      \n",
    "\n",
    "left_input = Input(shape=(250, 250, 3))                       \n",
    "right_input = Input(shape=(250, 250, 3))                      \n",
    "\n",
    "left_features = xception_base(left_input)                     \n",
    "right_input = xception_base(right_input)                      \n",
    "\n",
    "merged_features = layers.concatenate([left_features, right_input], axis=-1)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSPECTING AND MONITORING DEEP-LEARNING MODELS USING KERAS CALLBAACKS AND TENSORBOARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras.callbacks module includes a number of built-in callbacks (this is not an exhaustive list):\n",
    "    \n",
    "* `keras.callbacks.ModelCheckpoint`\n",
    "* `keras.callbacks.EarlyStopping`\n",
    "* `keras.callbacks.LearningRateScheduler`\n",
    "* `keras.callbacks.ReduceLROnPlateau`\n",
    "* `keras.callbacks.CSVLogger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "callbacks_list = [                              \n",
    "    keras.callbacks.EarlyStopping(              \n",
    "        monitor='acc',                          \n",
    "        patience=1,                             \n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(            \n",
    "        filepath='my_model.h5',                 \n",
    "        monitor='val_loss',                     \n",
    "        save_best_only=True,                    \n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])                  \n",
    "\n",
    "model.fit(x, y,             \n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          callbacks=callbacks_list, #!\n",
    "          validation_data=(x_val, y_val)) # Note that because the callback will monitor validation loss and validation accuracy, you need to pass validation_data to the call to fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can write your own callback. Callbacks are implemented by subclassing the class `keras.callbacks.Callback`. You can then implement any number of the following transparently named methods:\n",
    "\n",
    "* `on_epoch_begin`    1      \n",
    "* `on_epoch_end`      2    \n",
    "\n",
    "* `on_batch_begin`   3     \n",
    "* `on_batch_end`      4     \n",
    "\n",
    "* `on_train_begin`    5     \n",
    "* `on_train_end`      6 \n",
    "\n",
    "\n",
    "1 Called at the start of every epoch\n",
    "\n",
    "2 Called at the end of every epoch\n",
    "\n",
    "3 Called right before processing each batch\n",
    "\n",
    "4 Called right after processing each batch\n",
    "\n",
    "5 Called at the start of training\n",
    "\n",
    "6 Called at the end of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class ActivationLogger(keras.callbacks.Callback):\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model                                               \n",
    "        layer_outputs = [layer.output for layer in model.layers]\n",
    "        self.activations_model = keras.models.Model(model.input,layer_outputs)       \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is None:\n",
    "            raise RuntimeError('Requires validation_data.')\n",
    "\n",
    "        validation_sample = self.validation_data[0][0:1]                 \n",
    "        activations = self.activations_model.predict(validation_sample)\n",
    "        f = open('activations_at_epoch_' + str(epoch) + '.npz', 'w')     \n",
    "        np.savez(f, activations)                                         \n",
    "        f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to TensorBoard: the TensorFlow visualization framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hakan/.pyenv/versions/3.6.4/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 2000                                              \n",
    "max_len = 500                                                    \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hakan/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 500, 128)          256000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937\n",
      "Trainable params: 291,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 128,input_length=max_len,name='embed'))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir my_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 0.6326 - acc: 0.6522 - val_loss: 0.4155 - val_acc: 0.8302\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 63s 3ms/step - loss: 0.4367 - acc: 0.8149 - val_loss: 0.7051 - val_acc: 0.7046\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 77s 4ms/step - loss: 0.4001 - acc: 0.7883 - val_loss: 1.4597 - val_acc: 0.4790\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 89s 4ms/step - loss: 0.3467 - acc: 0.7799 - val_loss: 0.4892 - val_acc: 0.7716\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 71s 4ms/step - loss: 0.3129 - acc: 0.7148 - val_loss: 0.5548 - val_acc: 0.7030\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.2696 - acc: 0.6955 - val_loss: 0.5955 - val_acc: 0.6504\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 65s 3ms/step - loss: 0.2280 - acc: 0.6541 - val_loss: 0.6513 - val_acc: 0.6030\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 65s 3ms/step - loss: 0.2013 - acc: 0.5847 - val_loss: 0.7549 - val_acc: 0.5178\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 78s 4ms/step - loss: 0.1947 - acc: 0.5283 - val_loss: 0.7756 - val_acc: 0.4794\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 64s 3ms/step - loss: 0.1564 - acc: 0.4979 - val_loss: 1.2129 - val_acc: 0.3716\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 59s 3ms/step - loss: 0.1275 - acc: 0.3877 - val_loss: 0.9043 - val_acc: 0.3624\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.1217 - acc: 0.3419 - val_loss: 1.5910 - val_acc: 0.2836\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 59s 3ms/step - loss: 0.1163 - acc: 0.2989 - val_loss: 1.0122 - val_acc: 0.3080\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 59s 3ms/step - loss: 0.1138 - acc: 0.2625 - val_loss: 1.0959 - val_acc: 0.2842\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 59s 3ms/step - loss: 0.1131 - acc: 0.2402 - val_loss: 1.0845 - val_acc: 0.2786\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.1031 - acc: 0.2061 - val_loss: 1.1259 - val_acc: 0.2516\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 59s 3ms/step - loss: 0.1030 - acc: 0.1766 - val_loss: 1.1255 - val_acc: 0.2562\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 59s 3ms/step - loss: 0.1015 - acc: 0.1600 - val_loss: 1.1783 - val_acc: 0.2368\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 59s 3ms/step - loss: 0.1041 - acc: 0.1502 - val_loss: 1.1706 - val_acc: 0.2388\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 59s 3ms/step - loss: 0.1013 - acc: 0.1466 - val_loss: 1.1894 - val_acc: 0.2320\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='my_log_dir',                  \n",
    "        histogram_freq=1,                      \n",
    "        embeddings_freq=1,                     \n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_visual_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('trained_visual_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lunch `tensorboard --logdir=my_log_dir` from the command line. You can then browse to http://localhost:6006 and look at your model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True, to_file='model_shape.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "img=mpimg.imread('model.png')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
